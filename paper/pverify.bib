@misc{google_gemini25pro,
  author       = {{Google DeepMind}},
  title        = {Gemini 2.5 Pro},
  year         = {2025},
  howpublished = {\url{https://ai.google.dev/gemini-api/docs/models}},
  note         = {Multimodal large language model},
  urldate      = {2025-11-23}
}
@misc{openai_gpt5_system_card_2025,
  author      = {{OpenAI}},
  title       = {GPT-5 System Card},
  institution = {OpenAI},
  year        = {2025},
  month       = aug,
  url         = {https://cdn.openai.com/gpt-5-system-card.pdf},
  urldate     = {2025-11-23}
}
@misc{deepseek-ai_deepseek-r1_2025,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	shorttitle = {{DeepSeek}-{R1}},
	url = {/},
	doi = {10.48550/arXiv.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	urldate = {2025-01-23},
	publisher = {arXiv},
	author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.12948 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/LTIYGE22/DeepSeek-AI et al. - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/C6SW5MTI/2501.html:text/html},
}

@misc{kwa_measuring_2025,
	title = {Measuring {AI} {Ability} to {Complete} {Long} {Tasks}},
	url = {http://arxiv.org/abs/2503.14499},
	doi = {10.48550/arXiv.2503.14499},
	abstract = {Despite rapid progress on AI benchmarks, the real-world meaning of benchmark performance remains unclear. To quantify the capabilities of AI systems in terms of human capabilities, we propose a new metric: 50\%-task-completion time horizon. This is the time humans typically take to complete tasks that AI models can complete with 50\% success rate. We first timed humans with relevant domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet have a 50\% time horizon of around 50 minutes. Furthermore, frontier AI time horizon has been doubling approximately every seven months since 2019, though the trend may have accelerated in 2024. The increase in AI models' time horizons seems to be primarily driven by greater reliability and ability to adapt to mistakes, combined with better logical reasoning and tool use capabilities. We discuss the limitations of our results -- including their degree of external validity -- and the implications of increased autonomy for dangerous capabilities. If these results generalize to real-world software tasks, extrapolation of this trend predicts that within 5 years, AI systems will be capable of automating many software tasks that currently take humans a month.},
	urldate = {2025-03-21},
	publisher = {arXiv},
	author = {Kwa, Thomas and West, Ben and Becker, Joel and Deng, Amy and Garcia, Katharyn and Hasin, Max and Jawhar, Sami and Kinniment, Megan and Rush, Nate and Arx, Sydney Von and Bloom, Ryan and Broadley, Thomas and Du, Haoxing and Goodrich, Brian and Jurkovic, Nikola and Miles, Luke Harold and Nix, Seraphina and Lin, Tao and Parikh, Neev and Rein, David and Sato, Lucas Jun Koba and Wijk, Hjalmar and Ziegler, Daniel M. and Barnes, Elizabeth and Chan, Lawrence},
	month = mar,
	year = {2025},
	note = {arXiv:2503.14499 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/7XQX6583/Kwa et al. - 2025 - Measuring AI Ability to Complete Long Tasks.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/PKAANKQS/2503.html:text/html},
}

@misc{openai_openai_2024,
	title = {{OpenAI} o1 {System} {Card}},
	url = {http://arxiv.org/abs/2412.16720},
	doi = {10.48550/arXiv.2412.16720},
	abstract = {The o1 model series is trained with large-scale reinforcement learning to reason using chain of thought. These advanced reasoning capabilities provide new avenues for improving the safety and robustness of our models. In particular, our models can reason about our safety policies in context when responding to potentially unsafe prompts, through deliberative alignment. This leads to state-of-the-art performance on certain benchmarks for risks such as generating illicit advice, choosing stereotyped responses, and succumbing to known jailbreaks. Training models to incorporate a chain of thought before answering has the potential to unlock substantial benefits, while also increasing potential risks that stem from heightened intelligence. Our results underscore the need for building robust alignment methods, extensively stress-testing their efficacy, and maintaining meticulous risk management protocols. This report outlines the safety work carried out for the OpenAI o1 and OpenAI o1-mini models, including safety evaluations, external red teaming, and Preparedness Framework evaluations.},
	urldate = {2025-03-23},
	publisher = {arXiv},
	author = {OpenAI and Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and Iftimie, Alex and Karpenko, Alex and Passos, Alex Tachard and Neitz, Alexander and Prokofiev, Alexander and Wei, Alexander and Tam, Allison and Bennett, Ally and Kumar, Ananya and Saraiva, Andre and Vallone, Andrea and Duberstein, Andrew and Kondrich, Andrew and Mishchenko, Andrey and Applebaum, Andy and Jiang, Angela and Nair, Ashvin and Zoph, Barret and Ghorbani, Behrooz and Rossen, Ben and Sokolowsky, Benjamin and Barak, Boaz and McGrew, Bob and Minaiev, Borys and Hao, Botao and Baker, Bowen and Houghton, Brandon and McKinzie, Brandon and Eastman, Brydon and Lugaresi, Camillo and Bassin, Cary and Hudson, Cary and Li, Chak Ming and Bourcy, Charles de and Voss, Chelsea and Shen, Chen and Zhang, Chong and Koch, Chris and Orsinger, Chris and Hesse, Christopher and Fischer, Claudia and Chan, Clive and Roberts, Dan and Kappler, Daniel and Levy, Daniel and Selsam, Daniel and Dohan, David and Farhi, David and Mely, David and Robinson, David and Tsipras, Dimitris and Li, Doug and Oprica, Dragos and Freeman, Eben and Zhang, Eddie and Wong, Edmund and Proehl, Elizabeth and Cheung, Enoch and Mitchell, Eric and Wallace, Eric and Ritter, Erik and Mays, Evan and Wang, Fan and Such, Felipe Petroski and Raso, Filippo and Leoni, Florencia and Tsimpourlas, Foivos and Song, Francis and Lohmann, Fred von and Sulit, Freddie and Salmon, Geoff and Parascandolo, Giambattista and Chabot, Gildas and Zhao, Grace and Brockman, Greg and Leclerc, Guillaume and Salman, Hadi and Bao, Haiming and Sheng, Hao and Andrin, Hart and Bagherinezhad, Hessam and Ren, Hongyu and Lightman, Hunter and Chung, Hyung Won and Kivlichan, Ian and O'Connell, Ian and Osband, Ian and Gilaberte, Ignasi Clavera and Akkaya, Ilge and Kostrikov, Ilya and Sutskever, Ilya and Kofman, Irina and Pachocki, Jakub and Lennon, James and Wei, Jason and Harb, Jean and Twore, Jerry and Feng, Jiacheng and Yu, Jiahui and Weng, Jiayi and Tang, Jie and Yu, Jieqi and Candela, Joaquin Quiñonero and Palermo, Joe and Parish, Joel and Heidecke, Johannes and Hallman, John and Rizzo, John and Gordon, Jonathan and Uesato, Jonathan and Ward, Jonathan and Huizinga, Joost and Wang, Julie and Chen, Kai and Xiao, Kai and Singhal, Karan and Nguyen, Karina and Cobbe, Karl and Shi, Katy and Wood, Kayla and Rimbach, Kendra and Gu-Lemberg, Keren and Liu, Kevin and Lu, Kevin and Stone, Kevin and Yu, Kevin and Ahmad, Lama and Yang, Lauren and Liu, Leo and Maksin, Leon and Ho, Leyton and Fedus, Liam and Weng, Lilian and Li, Linden and McCallum, Lindsay and Held, Lindsey and Kuhn, Lorenz and Kondraciuk, Lukas and Kaiser, Lukasz and Metz, Luke and Boyd, Madelaine and Trebacz, Maja and Joglekar, Manas and Chen, Mark and Tintor, Marko and Meyer, Mason and Jones, Matt and Kaufer, Matt and Schwarzer, Max and Shah, Meghan and Yatbaz, Mehmet and Guan, Melody Y. and Xu, Mengyuan and Yan, Mengyuan and Glaese, Mia and Chen, Mianna and Lampe, Michael and Malek, Michael and Wang, Michele and Fradin, Michelle and McClay, Mike and Pavlov, Mikhail and Wang, Miles and Wang, Mingxuan and Murati, Mira and Bavarian, Mo and Rohaninejad, Mostafa and McAleese, Nat and Chowdhury, Neil and Chowdhury, Neil and Ryder, Nick and Tezak, Nikolas and Brown, Noam and Nachum, Ofir and Boiko, Oleg and Murk, Oleg and Watkins, Olivia and Chao, Patrick and Ashbourne, Paul and Izmailov, Pavel and Zhokhov, Peter and Dias, Rachel and Arora, Rahul and Lin, Randall and Lopes, Rapha Gontijo and Gaon, Raz and Miyara, Reah and Leike, Reimar and Hwang, Renny and Garg, Rhythm and Brown, Robin and James, Roshan and Shu, Rui and Cheu, Ryan and Greene, Ryan and Jain, Saachi and Altman, Sam and Toizer, Sam and Toyer, Sam and Miserendino, Samuel and Agarwal, Sandhini and Hernandez, Santiago and Baker, Sasha and McKinney, Scott and Yan, Scottie and Zhao, Shengjia and Hu, Shengli and Santurkar, Shibani and Chaudhuri, Shraman Ray and Zhang, Shuyuan and Fu, Siyuan and Papay, Spencer and Lin, Steph and Balaji, Suchir and Sanjeev, Suvansh and Sidor, Szymon and Broda, Tal and Clark, Aidan and Wang, Tao and Gordon, Taylor and Sanders, Ted and Patwardhan, Tejal and Sottiaux, Thibault and Degry, Thomas and Dimson, Thomas and Zheng, Tianhao and Garipov, Timur and Stasi, Tom and Bansal, Trapit and Creech, Trevor and Peterson, Troy and Eloundou, Tyna and Qi, Valerie and Kosaraju, Vineet and Monaco, Vinnie and Pong, Vitchyr and Fomenko, Vlad and Zheng, Weiyi and Zhou, Wenda and McCabe, Wes and Zaremba, Wojciech and Dubois, Yann and Lu, Yinghai and Chen, Yining and Cha, Young and Bai, Yu and He, Yuchen and Zhang, Yuchen and Wang, Yunyun and Shao, Zheng and Li, Zhuohan},
	month = dec,
	year = {2024},
	note = {arXiv:2412.16720 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/X26DM8SL/OpenAI et al. - 2024 - OpenAI o1 System Card.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/QSYKUHK2/2412.html:text/html},
}

@misc{shi_heimdall_2025,
	title = {Heimdall: test-time scaling on the generative verification},
	shorttitle = {Heimdall},
	url = {http://arxiv.org/abs/2504.10337},
	doi = {10.48550/arXiv.2504.10337},
	abstract = {An AI system can create and maintain knowledge only to the extent that it can verify that knowledge itself. Recent work on long Chain-of-Thought reasoning has demonstrated great potential of LLMs on solving competitive problems, but their verification ability remains to be weak and not sufficiently investigated. In this paper, we propose Heimdall, the long CoT verification LLM that can accurately judge the correctness of solutions. With pure reinforcement learning, we boost the verification accuracy from 62.5\% to 94.5\% on competitive math problems. By scaling with repeated sampling, the accuracy further increases to 97.5\%. Through human evaluation, Heimdall demonstrates impressive generalization capabilities, successfully detecting most issues in challenging math proofs, the type of which is not included during training. Furthermore, we propose Pessimistic Verification to extend the functionality of Heimdall to scaling up the problem solving. It calls Heimdall to judge the solutions from a solver model and based on the pessimistic principle, selects the most likely correct solution with the least uncertainty. Taking DeepSeek-R1-Distill-Qwen-32B as the solver model, Pessimistic Verification improves the solution accuracy on AIME2025 from 54.2\% to 70.0\% with 16x compute budget and to 83.3\% with more compute budget. With the stronger solver Gemini 2.5 Pro, the score reaches 93.0\%. Finally, we prototype an automatic knowledge discovery system, a ternary system where one poses questions, another provides solutions, and the third verifies the solutions. Using the data synthesis work NuminaMath for the first two components, Heimdall effectively identifies problematic records within the dataset and reveals that nearly half of the data is flawed, which interestingly aligns with the recent ablation studies from NuminaMath.},
	urldate = {2025-04-18},
	publisher = {arXiv},
	author = {Shi, Wenlei and Jin, Xing},
	month = apr,
	year = {2025},
	note = {arXiv:2504.10337 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/V3BRTHJG/Shi and Jin - 2025 - Heimdall test-time scaling on the generative verification.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/4LE2DI27/2504.html:text/html},
}

@misc{zuo_ttrl_2025,
	title = {{TTRL}: {Test}-{Time} {Reinforcement} {Learning}},
	shorttitle = {{TTRL}},
	url = {http://arxiv.org/abs/2504.16084},
	doi = {10.48550/arXiv.2504.16084},
	abstract = {This paper investigates Reinforcement Learning (RL) on data without explicit labels for reasoning tasks in Large Language Models (LLMs). The core challenge of the problem is reward estimation during inference while not having access to ground-truth information. While this setting appears elusive, we find that common practices in Test-Time Scaling (TTS), such as majority voting, yield surprisingly effective rewards suitable for driving RL training. In this work, we introduce Test-Time Reinforcement Learning (TTRL), a novel method for training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs by utilizing the priors in the pre-trained models. Our experiments demonstrate that TTRL consistently improves performance across a variety of tasks and models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by approximately 159\% on the AIME 2024 with only unlabeled test data. Furthermore, although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated performance to consistently surpass the upper limit of the initial model, and approach the performance of models trained directly on test data with ground-truth labels. Our experimental findings validate the general effectiveness of TTRL across various tasks, and highlight TTRL's potential for broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL},
	urldate = {2025-04-24},
	publisher = {arXiv},
	author = {Zuo, Yuxin and Zhang, Kaiyan and Qu, Shang and Sheng, Li and Zhu, Xuekai and Qi, Biqing and Sun, Youbang and Cui, Ganqu and Ding, Ning and Zhou, Bowen},
	month = apr,
	year = {2025},
	note = {arXiv:2504.16084 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/TTMM7T6Q/Zuo et al. - 2025 - TTRL Test-Time Reinforcement Learning.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/KC27UB9M/2504.html:text/html},
}

@misc{chen_rm-r1_2025,
	title = {{RM}-{R1}: {Reward} {Modeling} as {Reasoning}},
	shorttitle = {{RM}-{R1}},
	url = {http://arxiv.org/abs/2505.02387},
	doi = {10.48550/arXiv.2505.02387},
	abstract = {Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8\%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.},
	urldate = {2025-05-11},
	publisher = {arXiv},
	author = {Chen, Xiusi and Li, Gaotang and Wang, Ziqi and Jin, Bowen and Qian, Cheng and Wang, Yu and Wang, Hongru and Zhang, Yu and Zhang, Denghui and Zhang, Tong and Tong, Hanghang and Ji, Heng},
	month = may,
	year = {2025},
	note = {arXiv:2505.02387 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 23 pages, 7 figures},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/QI7TPRYC/Chen et al. - 2025 - RM-R1 Reward Modeling as Reasoning.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/LNFT44MP/2505.html:text/html},
}

@misc{huang_gemini_2025,
	title = {Gemini 2.5 {Pro} {Capable} of {Winning} {Gold} at {IMO} 2025},
	url = {http://arxiv.org/abs/2507.15855},
	doi = {10.48550/arXiv.2507.15855},
	abstract = {The International Mathematical Olympiad (IMO) poses uniquely challenging problems requiring deep insight, creativity, and formal reasoning. While Large Language Models (LLMs) perform well on mathematical benchmarks like AIME, they struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly released IMO 2025 problems, avoiding data contamination. Using a self-verification pipeline with careful prompt design, 5 (out of 6) problems are solved correctly. This result underscores the importance of developing optimal strategies to harness the full potential of powerful LLMs for complex reasoning tasks.},
	urldate = {2025-07-28},
	publisher = {arXiv},
	author = {Huang, Yichen and Yang, Lin F.},
	month = jul,
	year = {2025},
	note = {arXiv:2507.15855 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/7PU58IZK/Huang and Yang - 2025 - Gemini 2.5 Pro Capable of Winning Gold at IMO 2025.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/3M8HS55Q/2507.html:text/html},
}

@misc{team_kimi_2025,
	title = {Kimi {K2}: {Open} {Agentic} {Intelligence}},
	shorttitle = {Kimi {K2}},
	url = {http://arxiv.org/abs/2507.20534},
	doi = {10.48550/arXiv.2507.20534},
	abstract = {We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments. Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.},
	urldate = {2025-08-04},
	publisher = {arXiv},
	author = {Team, Kimi and Bai, Yifan and Bao, Yiping and Chen, Guanduo and Chen, Jiahao and Chen, Ningxin and Chen, Ruijue and Chen, Yanru and Chen, Yuankun and Chen, Yutian and Chen, Zhuofu and Cui, Jialei and Ding, Hao and Dong, Mengnan and Du, Angang and Du, Chenzhuang and Du, Dikang and Du, Yulun and Fan, Yu and Feng, Yichen and Fu, Kelin and Gao, Bofei and Gao, Hongcheng and Gao, Peizhong and Gao, Tong and Gu, Xinran and Guan, Longyu and Guo, Haiqing and Guo, Jianhang and Hu, Hao and Hao, Xiaoru and He, Tianhong and He, Weiran and He, Wenyang and Hong, Chao and Hu, Yangyang and Hu, Zhenxing and Huang, Weixiao and Huang, Zhiqi and Huang, Zihao and Jiang, Tao and Jiang, Zhejun and Jin, Xinyi and Kang, Yongsheng and Lai, Guokun and Li, Cheng and Li, Fang and Li, Haoyang and Li, Ming and Li, Wentao and Li, Yanhao and Li, Yiwei and Li, Zhaowei and Li, Zheming and Lin, Hongzhan and Lin, Xiaohan and Lin, Zongyu and Liu, Chengyin and Liu, Chenyu and Liu, Hongzhang and Liu, Jingyuan and Liu, Junqi and Liu, Liang and Liu, Shaowei and Liu, T. Y. and Liu, Tianwei and Liu, Weizhou and Liu, Yangyang and Liu, Yibo and Liu, Yiping and Liu, Yue and Liu, Zhengying and Lu, Enzhe and Lu, Lijun and Ma, Shengling and Ma, Xinyu and Ma, Yingwei and Mao, Shaoguang and Mei, Jie and Men, Xin and Miao, Yibo and Pan, Siyuan and Peng, Yebo and Qin, Ruoyu and Qu, Bowen and Shang, Zeyu and Shi, Lidong and Shi, Shengyuan and Song, Feifan and Su, Jianlin and Su, Zhengyuan and Sun, Xinjie and Sung, Flood and Tang, Heyi and Tao, Jiawen and Teng, Qifeng and Wang, Chensi and Wang, Dinglu and Wang, Feng and Wang, Haiming and Wang, Jianzhou and Wang, Jiaxing and Wang, Jinhong and Wang, Shengjie and Wang, Shuyi and Wang, Yao and Wang, Yejie and Wang, Yiqin and Wang, Yuxin and Wang, Yuzhi and Wang, Zhaoji and Wang, Zhengtao and Wang, Zhexu and Wei, Chu and Wei, Qianqian and Wu, Wenhao and Wu, Xingzhe and Wu, Yuxin and Xiao, Chenjun and Xie, Xiaotong and Xiong, Weimin and Xu, Boyu and Xu, Jing and Xu, Jinjing and Xu, L. H. and Xu, Lin and Xu, Suting and Xu, Weixin and Xu, Xinran and Xu, Yangchuan and Xu, Ziyao and Yan, Junjie and Yan, Yuzi and Yang, Xiaofei and Yang, Ying and Yang, Zhen and Yang, Zhilin and Yang, Zonghan and Yao, Haotian and Yao, Xingcheng and Ye, Wenjie and Ye, Zhuorui and Yin, Bohong and Yu, Longhui and Yuan, Enming and Yuan, Hongbang and Yuan, Mengjie and Zhan, Haobing and Zhang, Dehao and Zhang, Hao and Zhang, Wanlu and Zhang, Xiaobin and Zhang, Yangkun and Zhang, Yizhi and Zhang, Yongting and Zhang, Yu and Zhang, Yutao and Zhang, Yutong and Zhang, Zheng and Zhao, Haotian and Zhao, Yikai and Zheng, Huabin and Zheng, Shaojie and Zhou, Jianren and Zhou, Xinyu and Zhou, Zaida and Zhu, Zhen and Zhuang, Weiyu and Zu, Xinxing},
	month = jul,
	year = {2025},
	note = {arXiv:2507.20534 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: tech report of Kimi K2},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/PZVHZZBN/Team et al. - 2025 - Kimi K2 Open Agentic Intelligence.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/CDFDG66Y/2507.html:text/html},
}

@misc{chen_seed-prover_2025,
	title = {Seed-{Prover}: {Deep} and {Broad} {Reasoning} for {Automated} {Theorem} {Proving}},
	shorttitle = {Seed-{Prover}},
	url = {http://arxiv.org/abs/2507.23726},
	doi = {10.48550/arXiv.2507.23726},
	abstract = {LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose {\textbackslash}textbf\{Seed-Prover\}, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves \$78.1{\textbackslash}\%\$ of formalized past IMO problems, saturates MiniF2F, and achieves over 50{\textbackslash}\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine {\textbackslash}textbf\{Seed-Geometry\}, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.},
	urldate = {2025-08-06},
	publisher = {arXiv},
	author = {Chen, Luoxin and Gu, Jinming and Huang, Liankai and Huang, Wenhao and Jiang, Zhicheng and Jie, Allan and Jin, Xiaoran and Jin, Xing and Li, Chenggang and Ma, Kaijing and Ren, Cheng and Shen, Jiawei and Shi, Wenlei and Sun, Tong and Sun, He and Wang, Jiahui and Wang, Siran and Wang, Zhihong and Wei, Chenrui and Wei, Shufa and Wu, Yonghui and Wu, Yuchen and Xia, Yihang and Xin, Huajian and Yang, Fan and Ying, Huaiyuan and Yuan, Hongyi and Yuan, Zheng and Zhan, Tianyang and Zhang, Chi and Zhang, Yue and Zhang, Ge and Zhao, Tianyun and Zhao, Jianqiu and Zhou, Yichi and Zhu, Thomas Hanwen},
	month = aug,
	year = {2025},
	note = {arXiv:2507.23726 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/I9ALGHQR/Chen et al. - 2025 - Seed-Prover Deep and Broad Reasoning for Automated Theorem Proving.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/9V378K3A/2507.html:text/html},
}

@misc{sinha_illusion_2025,
	title = {The {Illusion} of {Diminishing} {Returns}: {Measuring} {Long} {Horizon} {Execution} in {LLMs}},
	shorttitle = {The {Illusion} of {Diminishing} {Returns}},
	url = {http://arxiv.org/abs/2509.09677},
	doi = {10.48550/arXiv.2509.09677},
	abstract = {Does continued scaling of large language models (LLMs) yield diminishing returns? Real-world value often stems from the length of task an agent can complete. We start this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponential improvements in the length of a task a model can successfully complete. Then, we argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. We propose isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. We find that larger models can correctly execute significantly more turns even when small models have 100{\textbackslash}\% single-turn accuracy. We observe that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations -- curiously, we observe a self-conditioning effect -- models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. In contrast, recent thinking models do not self-condition, and can also execute much longer tasks in a single turn. We conclude by benchmarking frontier thinking models on the length of task they can execute in a single turn. Overall, by focusing on the ability to execute, we hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks.},
	urldate = {2025-09-18},
	publisher = {arXiv},
	author = {Sinha, Akshit and Arun, Arvindh and Goel, Shashwat and Staab, Steffen and Geiping, Jonas},
	month = sep,
	year = {2025},
	note = {arXiv:2509.09677 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/RWYNXKC9/Sinha et al. - 2025 - The Illusion of Diminishing Returns Measuring Long Horizon Execution in LLMs.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/5MCLAI8K/2509.html:text/html},
}

@misc{yang_qwen3_2025,
	title = {Qwen3 {Technical} {Report}},
	url = {http://arxiv.org/abs/2505.09388},
	doi = {10.48550/arXiv.2505.09388},
	abstract = {In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.},
	urldate = {2025-09-18},
	publisher = {arXiv},
	author = {Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and Zheng, Chujie and Liu, Dayiheng and Zhou, Fan and Huang, Fei and Hu, Feng and Ge, Hao and Wei, Haoran and Lin, Huan and Tang, Jialong and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin and Yang, Jiaxi and Zhou, Jing and Zhou, Jingren and Lin, Junyang and Dang, Kai and Bao, Keqin and Yang, Kexin and Yu, Le and Deng, Lianghao and Li, Mei and Xue, Mingfeng and Li, Mingze and Zhang, Pei and Wang, Peng and Zhu, Qin and Men, Rui and Gao, Ruize and Liu, Shixuan and Luo, Shuang and Li, Tianhao and Tang, Tianyi and Yin, Wenbiao and Ren, Xingzhang and Wang, Xinyu and Zhang, Xinyu and Ren, Xuancheng and Fan, Yang and Su, Yang and Zhang, Yichang and Zhang, Yinger and Wan, Yu and Liu, Yuqiong and Wang, Zekun and Cui, Zeyu and Zhang, Zhenru and Zhou, Zhipeng and Qiu, Zihan},
	month = may,
	year = {2025},
	note = {arXiv:2505.09388 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/VB8QM9EF/Yang et al. - 2025 - Qwen3 Technical Report.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/FJD34254/2505.html:text/html},
}

@misc{yu_rlpr_2025,
	title = {{RLPR}: {Extrapolating} {RLVR} to {General} {Domains} without {Verifiers}},
	shorttitle = {{RLPR}},
	url = {http://arxiv.org/abs/2506.18254},
	doi = {10.48550/arXiv.2506.18254},
	abstract = {Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising potential in advancing the reasoning capabilities of LLMs. However, its success remains largely confined to mathematical and code domains. This primary limitation stems from the heavy reliance on domain-specific verifiers, which results in prohibitive complexity and limited scalability. To address the challenge, our key observation is that LLM's intrinsic probability of generating a correct free-form answer directly indicates its own evaluation of the reasoning reward (i.e., how well the reasoning process leads to the correct answer). Building on this insight, we propose RLPR, a simple verifier-free framework that extrapolates RLVR to broader general domains. RLPR uses the LLM's own token probability scores for reference answers as the reward signal and maximizes the expected reward during training. We find that addressing the high variance of this noisy probability reward is crucial to make it work, and propose prob-to-reward and stabilizing methods to ensure a precise and stable reward from LLM intrinsic probabilities. Comprehensive experiments in four general-domain benchmarks and three mathematical benchmarks show that RLPR consistently improves reasoning capabilities in both areas for Gemma, Llama, and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6 points on TheoremQA and 7.5 points on Minerva, and even surpasses strong verifier-model-dependent approaches General-Reasoner by 1.6 average points across seven benchmarks.},
	urldate = {2025-09-24},
	publisher = {arXiv},
	author = {Yu, Tianyu and Ji, Bo and Wang, Shouli and Yao, Shu and Wang, Zefan and Cui, Ganqu and Yuan, Lifan and Ding, Ning and Yao, Yuan and Liu, Zhiyuan and Sun, Maosong and Chua, Tat-Seng},
	month = jun,
	year = {2025},
	note = {arXiv:2506.18254 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Project Website: https://github.com/openbmb/RLPR},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/FISPLLCW/Yu et al. - 2025 - RLPR Extrapolating RLVR to General Domains without Verifiers.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/LJHAU66F/2506.html:text/html},
}

@misc{varambally_hilbert_2025,
	title = {Hilbert: {Recursively} {Building} {Formal} {Proofs} with {Informal} {Reasoning}},
	shorttitle = {Hilbert},
	url = {http://arxiv.org/abs/2509.22819},
	doi = {10.48550/arXiv.2509.22819},
	abstract = {Large Language Models (LLMs) demonstrate impressive mathematical reasoning abilities, but their solutions frequently contain errors that cannot be automatically verified. Formal theorem proving systems such as Lean 4 offer automated verification with complete accuracy, motivating recent efforts to build specialized prover LLMs that generate verifiable proofs in formal languages. However, a significant gap remains: current prover LLMs solve substantially fewer problems than general-purpose LLMs operating in natural language. We introduce Hilbert, an agentic framework that bridges this gap by combining the complementary strengths of informal reasoning and formal verification. Our system orchestrates four components: an informal LLM that excels at mathematical reasoning, a specialized prover LLM optimized for Lean 4 tactics, a formal verifier, and a semantic theorem retriever. Given a problem that the prover is unable to solve, Hilbert employs recursive decomposition to split the problem into subgoals that it solves with the prover or reasoner LLM. It leverages verifier feedback to refine incorrect proofs as necessary. Experimental results demonstrate that Hilbert substantially outperforms existing approaches on key benchmarks, achieving 99.2\% on miniF2F, 6.6\% points above the best publicly available method. Hilbert achieves the best known result on PutnamBench. It solves 462/660 problems (70.0\%), outperforming proprietary approaches like SeedProver (50.4\%) and achieving a 422\% improvement over the best publicly available baseline. Thus, Hilbert effectively narrows the gap between informal reasoning and formal proof generation.},
	urldate = {2025-11-04},
	publisher = {arXiv},
	author = {Varambally, Sumanth and Voice, Thomas and Sun, Yanchao and Chen, Zhifeng and Yu, Rose and Ye, Ke},
	month = sep,
	year = {2025},
	note = {arXiv:2509.22819 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Formal Languages and Automata Theory},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/54S565N9/Varambally et al. - 2025 - Hilbert Recursively Building Formal Proofs with Informal Reasoning.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/U5D3KYY9/2509.html:text/html},
}

@misc{pandit_hard2verify_2025,
	title = {{Hard2Verify}: {A} {Step}-{Level} {Verification} {Benchmark} for {Open}-{Ended} {Frontier} {Math}},
	shorttitle = {{Hard2Verify}},
	url = {http://arxiv.org/abs/2510.13744},
	doi = {10.48550/arXiv.2510.13744},
	abstract = {Large language model (LLM)-based reasoning systems have recently achieved gold medal-level performance in the IMO 2025 competition, writing mathematical proofs where, to receive full credit, each step must be not only correct but also sufficiently supported. To train LLM-based reasoners in such challenging, open-ended settings, strong verifiers capable of catching step-level mistakes are necessary prerequisites. We introduce Hard2Verify, a human-annotated, step-level verification benchmark produced with over 500 hours of human labor. Hard2Verify is designed to rigorously assess step-level verifiers at the frontier: Verifiers must provide step-level annotations or identify the first error in responses generated by frontier LLMs for very recent, challenging, and open-ended math questions. We evaluate 29 generative critics and process reward models, demonstrating that, beyond a few standouts, open-source verifiers lag closed source models. We subsequently analyze what drives poor performance in step-level verification, the impacts of scaling verifier compute, as well as fundamental questions such as self-verification and verification-generation dynamics.},
	urldate = {2025-11-07},
	publisher = {arXiv},
	author = {Pandit, Shrey and Xu, Austin and Nguyen, Xuan-Phi and Ming, Yifei and Xiong, Caiming and Joty, Shafiq},
	month = oct,
	year = {2025},
	note = {arXiv:2510.13744 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 21 pages, 8 figures, 5 tables},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/GPXD6ZM6/Pandit et al. - 2025 - Hard2Verify A Step-Level Verification Benchmark for Open-Ended Frontier Math.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/KD5GGGMB/2510.html:text/html},
}

@misc{dekoninck_open_2025,
	title = {The {Open} {Proof} {Corpus}: {A} {Large}-{Scale} {Study} of {LLM}-{Generated} {Mathematical} {Proofs}},
	shorttitle = {The {Open} {Proof} {Corpus}},
	url = {http://arxiv.org/abs/2506.21621},
	doi = {10.48550/arXiv.2506.21621},
	abstract = {In recent months, large language models (LLMs) have made significant progress in mathematical proof generation, but further advancement is hindered by the lack of a large-scale, high-quality dataset of human-evaluated proofs. While expensive to create, such a dataset is essential for driving improvements in training and enabling a rigorous analysis of proof generation capabilities. In this work, we present the Open Proof Corpus (OPC), a dataset comprising over 5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was specifically designed for broad applicability and downstream usage in proof generation research and is the first to include a substantial number of correct, LLM-generated solutions to problems from prestigious mathematics competitions such as the USAMO and IMO. Using the OPC, we explore critical questions in automated proof generation: (1) the performance gap between natural language and formal proof generation, (2) the discrepancy between final-answer accuracy and full-proof validity, and (3) the impact of best-of-n selection on proof quality. Finally, to showcase the utility of the OPC, we finetune an 8B-parameter model on the dataset, obtaining a model that performs on par with the best model, Gemini-2.5-Pro, on the task of evaluating proof correctness.},
	urldate = {2025-11-11},
	publisher = {arXiv},
	author = {Dekoninck, Jasper and Petrov, Ivo and Minchev, Kristian and Balunovic, Mislav and Vechev, Martin and Marinov, Miroslav and Drencheva, Maria and Konova, Lyuba and Shumanov, Milen and Tsvetkov, Kaloyan and Drenchev, Nikolay and Todorov, Lazar and Nikolova, Kalina and Georgiev, Nikolay and Kalinkova, Vanesa and Ismoldayev, Margulan},
	month = jun,
	year = {2025},
	note = {arXiv:2506.21621 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/PKSXDLZ6/Dekoninck et al. - 2025 - The Open Proof Corpus A Large-Scale Study of LLM-Generated Mathematical Proofs.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/Y5UWSMJ9/2506.html:text/html},
}

@misc{xu_direct_2025,
	title = {Direct {Reasoning} {Optimization}: {LLMs} {Can} {Reward} {And} {Refine} {Their} {Own} {Reasoning} for {Open}-{Ended} {Tasks}},
	shorttitle = {Direct {Reasoning} {Optimization}},
	url = {http://arxiv.org/abs/2506.13351},
	doi = {10.48550/arXiv.2506.13351},
	abstract = {Recent advances in Large Language Models (LLMs) have showcased impressive reasoning abilities in structured tasks like mathematics and programming, largely driven by Reinforcement Learning with Verifiable Rewards (RLVR), which uses outcome-based signals that are scalable, effective, and robust against reward hacking. However, applying similar techniques to open-ended long-form reasoning tasks remains challenging due to the absence of generic, verifiable reward signals. To address this, we propose Direct Reasoning Optimization (DRO), a reinforcement learning framework for fine-tuning LLMs on open-ended, particularly long-form, reasoning tasks, guided by a new reward signal: the Reasoning Reflection Reward (R3). At its core, R3 selectively identifies and emphasizes key tokens in the reference outcome that reflect the influence of the model's preceding chain-of-thought reasoning, thereby capturing the consistency between reasoning and reference outcome at a fine-grained level. Crucially, R3 is computed internally using the same model being optimized, enabling a fully self-contained training setup. Additionally, we introduce a dynamic data filtering strategy based on R3 for open-ended reasoning tasks, reducing cost while improving downstream performance. We evaluate DRO on two diverse datasets -- ParaRev, a long-form paragraph revision task, and FinQA, a math-oriented QA benchmark -- and show that it consistently outperforms strong baselines while remaining broadly applicable across both open-ended and structured domains.},
	urldate = {2025-11-11},
	publisher = {arXiv},
	author = {Xu, Yifei and Chakraborty, Tusher and Sharma, Srinagesh and Nunes, Leonardo and Kıcıman, Emre and Lu, Songwu and Chandra, Ranveer},
	month = jun,
	year = {2025},
	note = {arXiv:2506.13351 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/5X29NVGH/Xu et al. - 2025 - Direct Reasoning Optimization LLMs Can Reward And Refine Their Own Reasoning for Open-Ended Tasks.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/I9DFM4R5/2506.html:text/html},
}

@misc{luong_towards_2025,
	title = {Towards {Robust} {Mathematical} {Reasoning}},
	url = {http://arxiv.org/abs/2511.01846},
	doi = {10.48550/arXiv.2511.01846},
	abstract = {Finding the right north-star metrics is highly critical for advancing the mathematical reasoning capabilities of foundation models, especially given that existing evaluations are either too easy or only focus on getting correct short answers. To address these issues, we present IMO-Bench, a suite of advanced reasoning benchmarks, vetted by a panel of top specialists and that specifically targets the level of the International Mathematical Olympiad (IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench first tests models on 400 diverse Olympiad problems with verifiable short answers. IMO-Proof Bench is the next-level evaluation for proof-writing capabilities, which includes both basic and advanced IMO level problems as well as detailed grading guidelines to facilitate automatic grading. These benchmarks played a crucial role in our historic achievement of the gold-level performance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our model achieved 80.0\% on IMO-AnswerBench and 65.7\% on the advanced IMO-Proof Bench, surpassing the best non-Gemini models by large margins of 6.9\% and 42.4\% respectively. We also showed that autograders built with Gemini reasoning correlate well with human evaluations and construct IMO-GradingBench, with 1000 human gradings on proofs, to enable further progress in automatic evaluation of long-form answers. We hope that IMO-Bench will help the community towards advancing robust mathematical reasoning and release it at https://imobench.github.io/.},
	urldate = {2025-11-13},
	publisher = {arXiv},
	author = {Luong, Thang and Hwang, Dawsen and Nguyen, Hoang H. and Ghiasi, Golnaz and Chervonyi, Yuri and Seo, Insuk and Kim, Junsu and Bingham, Garrett and Lee, Jonathan and Mishra, Swaroop and Zhai, Alex and Hu, Clara Huiyi and Michalewski, Henryk and Kim, Jimin and Ahn, Jeonghyun and Bae, Junhwi and Song, Xingyou and Trinh, Trieu H. and Le, Quoc V. and Jung, Junehyuk},
	month = nov,
	year = {2025},
	note = {arXiv:2511.01846 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: EMNLP 2025 (main conference), https://aclanthology.org/2025.emnlp-main.1794/},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/GZABJEDS/Luong et al. - 2025 - Towards Robust Mathematical Reasoning.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/2KPWPKMV/2511.html:text/html},
}

@misc{comanici_gemini_2025,
	title = {Gemini 2.5: {Pushing} the {Frontier} with {Advanced} {Reasoning}, {Multimodality}, {Long} {Context}, and {Next} {Generation} {Agentic} {Capabilities}},
	shorttitle = {Gemini 2.5},
	url = {http://arxiv.org/abs/2507.06261},
	doi = {10.48550/arXiv.2507.06261},
	abstract = {In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.},
	urldate = {2025-11-17},
	publisher = {arXiv},
	author = {Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and Marris, Luke and Petulla, Sam and Gaffney, Colin and Aharoni, Asaf and Lintz, Nathan and Pais, Tiago Cardal and Jacobsson, Henrik and Szpektor, Idan and Jiang, Nan-Jiang and Haridasan, Krishna and Omran, Ahmed and Saunshi, Nikunj and Bahri, Dara and Mishra, Gaurav and Chu, Eric and Boyd, Toby and Hekman, Brad and Parisi, Aaron and Zhang, Chaoyi and Kawintiranon, Kornraphop and Bedrax-Weiss, Tania and Wang, Oliver and Xu, Ya and Purkiss, Ollie and Mendlovic, Uri and Deutel, Ilaï and Nguyen, Nam and Langley, Adam and Korn, Flip and Rossazza, Lucia and Ramé, Alexandre and Waghmare, Sagar and Miller, Helen and Byrd, Nathan and Sheshan, Ashrith and Hadsell, Raia and Bhardwaj, Sangnie and Janus, Pawel and Rissa, Tero and Horgan, Dan and Abdagic, Alvin and Belenki, Lior and Allingham, James and Singh, Anima and Guidroz, Theo and Srinivasan, Srivatsan and Schmit, Herman and Chiafullo, Kristen and Elisseeff, Andre and Jha, Nilpa and Kolhar, Prateek and Berrada, Leonard and Ding, Frank and Si, Xiance and Mallick, Shrestha Basu and Och, Franz and Erell, Sofia and Ni, Eric and Latkar, Tejasi and Yang, Sherry and Sirkovic, Petar and Feng, Ziqiang and Leland, Robert and Hornung, Rachel and Wu, Gang and Blundell, Charles and Alvari, Hamidreza and Huang, Po-Sen and Yip, Cathy and Deur, Sanja and Liu, Li and Surita, Gabriela and Duque, Pablo and Damen, Dima and Jia, Johnson and Guez, Arthur and Mircea, Markus and Sinha, Animesh and Magni, Alberto and Stradomski, Paweł and Marian, Tal and Galić, Vlado and Chen, Wenhu and Husain, Hisham and Singhal, Achintya and Grewe, Dominik and Aubet, François-Xavier and Song, Shuang and Blanco, Lorenzo and Rechis, Leland and Ho, Lewis and Munoz, Rich and Zheng, Kelvin and Hamrick, Jessica and Mather, Kevin and Taitelbaum, Hagai and Rutherford, Eliza and Lei, Yun and Chen, Kuangyuan and Shukla, Anand and Moreira, Erica and Doi, Eric and Isik, Berivan and Shabat, Nir and Rogozińska, Dominika and Kolipaka, Kashyap and Chang, Jason and Vušak, Eugen and Venkatachary, Srinivasan and Noghabi, Shadi and Bharti, Tarun and Jun, Younghoon and Zaks, Aleksandr and Green, Simon and Challagundla, Jeshwanth and Wong, William and Mohammad, Muqthar and Hirsch, Dean and Cheng, Yong and Naim, Iftekhar and Proleev, Lev and Vincent, Damien and Singh, Aayush and Krikun, Maxim and Krishnan, Dilip and Ghahramani, Zoubin and Atias, Aviel and Aggarwal, Rajeev and Kirov, Christo and Vytiniotis, Dimitrios and Koh, Christy and Chronopoulou, Alexandra and Dogra, Pawan and Ion, Vlad-Doru and Tyen, Gladys and Lee, Jason and Weissenberger, Felix and Strohman, Trevor and Balakrishna, Ashwin and Rae, Jack and Velic, Marko and Liedekerke, Raoul de and Elyada, Oded and Yuan, Wentao and Liu, Canoee and Shani, Lior and Kishchenko, Sergey and Alessio, Bea and Li, Yandong and Song, Richard and Kwei, Sam and Jankowski, Orion and Pappu, Aneesh and Namiki, Youhei and Ma, Yenai and Tripuraneni, Nilesh and Cherry, Colin and Ikonomidis, Marissa and Ling, Yu-Cheng and Ji, Colin and Westberg, Beka and Wright, Auriel and Yu, Da and Parkinson, David and Ramaswamy, Swaroop and Connor, Jerome and Yeganeh, Soheil Hassas and Grover, Snchit and Kenwright, George and Litchev, Lubo and Apps, Chris and Tomala, Alex and Halim, Felix and Castro-Ros, Alex and Li, Zefei and Boral, Anudhyan and Sho, Pauline and Yarom, Michal and Malmi, Eric and Klinghoffer, David and Lin, Rebecca and Ansell, Alan and S, Pradeep Kumar and Zhao, Shubin and Zuo, Siqi and Santoro, Adam and Cheng, Heng-Tze and Demmessie, Solomon and Liu, Yuchi and Brichtova, Nicole and Culp, Allie and Braun, Nathaniel and Graur, Dan and Ng, Will and Mehta, Nikhil and Phillips, Aaron and Sundberg, Patrik and Godbole, Varun and Liu, Fangyu and Katariya, Yash and Rim, David and Seyedhosseini, Mojtaba and Ammirati, Sean and Valfridsson, Jonas and Malihi, Mahan and Knight, Timothy and Toor, Andeep and Lampe, Thomas and Ittycheriah, Abe and Chiang, Lewis and Yeung, Chak and Fréchette, Alexandre and Rao, Jinmeng and Wang, Huisheng and Srivastava, Himanshu and Zhang, Richard and Rhodes, Rocky and Brand, Ariel and Weesner, Dean and Figotin, Ilya and Gimeno, Felix and Fellinger, Rachana and Marcenac, Pierre and Leal, José and Marcus, Eyal and Cotruta, Victor and Cabrera, Rodrigo and Luo, Sheryl and Garrette, Dan and Axelrod, Vera and Baltateanu, Sorin and Barker, David and Chen, Dongkai and Toma, Horia and Ingram, Ben and Riesa, Jason and Kulkarni, Chinmay and Zhang, Yujing and Liu, Hongbin and Wang, Chao and Polacek, Martin and Wu, Will and Hui, Kai and Reyes, Adrian N. and Su, Yi and Barnes, Megan and Malhi, Ishaan and Siddiqui, Anfal and Feng, Qixuan and Damaschin, Mihai and Pighin, Daniele and Steiner, Andreas and Yang, Samuel and Boppana, Ramya Sree and Ivanov, Simeon and Kandoor, Arun and Shah, Aditya and Mujika, Asier and Huang, Da and Choquette-Choo, Christopher A. and Patel, Mohak and Yu, Tianhe and Creswell, Toni and Jerry and Liu and Barros, Catarina and Razeghi, Yasaman and Roy, Aurko and Culliton, Phil and Xiong, Binbin and Pan, Jiaqi and Strohmann, Thomas and Powell, Tolly and Seal, Babi and DeCarlo, Doug and Shyam, Pranav and Katircioglu, Kaan and Wang, Xuezhi and Hardin, Cassidy and Odisho, Immanuel and Broder, Josef and Chang, Oscar and Nair, Arun and Shtefan, Artem and O'Brien, Maura and Agarwal, Manu and Potluri, Sahitya and Goyal, Siddharth and Jhindal, Amit and Thakur, Saksham and Stuken, Yury and Lyon, James and Toutanova, Kristina and Feng, Fangxiaoyu and Wu, Austin and Horn, Ben and Wang, Alek and Cullum, Alex and Taubman, Gabe and Shrivastava, Disha and Shi, Chongyang and Tomlinson, Hamish and Patel, Roma and Tu, Tao and Oflazer, Ada Maksutaj and Pongetti, Francesco and Yang, Mingyao and Taïga, Adrien Ali and Perot, Vincent and Pierse, Nuo Wang and Han, Feng and Drori, Yoel and Iturrate, Iñaki and Chakrabarti, Ayan and Yeung, Legg and Dopson, Dave and Chen, Yi-ting and Kulshreshtha, Apoorv and Guo, Tongfei and Pham, Philip and Schuster, Tal and Chen, Junquan and Polozov, Alex and Xing, Jinwei and Zhou, Huanjie and Kacham, Praneeth and Kukliansky, Doron and Miech, Antoine and Yaroshenko, Sergey and Chi, Ed and Douglas, Sholto and Fei, Hongliang and Blondel, Mathieu and Myla, Preethi and Madmoni, Lior and Wu, Xing and Keysers, Daniel and Kjems, Kristian and Albuquerque, Isabela and Yu, Lijun and D'sa, Joel and Plantan, Michelle and Ionescu, Vlad and Elias, Jaume Sanchez and Gupta, Abhirut and Vuyyuru, Manish Reddy and Alcober, Fred and Zhou, Tong and Ji, Kaiyang and Hartmann, Florian and Puttagunta, Subha and Song, Hugo and Amid, Ehsan and Stefanoiu, Anca and Lee, Andrew and Pucciarelli, Paul and Wang, Emma and Raul, Amit and Petrov, Slav and Tian, Isaac and Anklin, Valentin and Nti, Nana and Gomes, Victor and Schumacher, Max and Vesom, Grace and Panagopoulos, Alex and Bousmalis, Konstantinos and Andor, Daniel and Jacob, Josh and Zhang, Yuan and Rosgen, Bill and Kecman, Matija and Tung, Matthew and Belias, Alexandra and Goodman, Noah and Covington, Paul and Wieder, Brian and Saxena, Nikita and Davoodi, Elnaz and Huang, Muhuan and Maddineni, Sharath and Roulet, Vincent and Campbell-Ajala, Folawiyo and Sessa, Pier Giuseppe and Xintian and Wu and Lai, Guangda and Collins, Paul and Haig, Alex and Sakenas, Vytenis and Xu, Xiaowei and Giustina, Marissa and Shafey, Laurent El and Charoenpanit, Pichi and Garg, Shefali and Ainslie, Joshua and Severson, Boone and Arenas, Montse Gonzalez and Pathak, Shreya and Rajayogam, Sujee and Feng, Jie and Bakker, Michiel and Li, Sheng and Wichers, Nevan and Rogers, Jamie and Geng, Xinyang and Li, Yeqing and Jagerman, Rolf and Jia, Chao and Olmert, Nadav and Sharon, David and Mauger, Matthew and Mariserla, Sandeep and Ma, Hongxu and Mohabey, Megha and Kim, Kyuyeun and Andreev, Alek and Pollom, Scott and Love, Juliette and Jain, Vihan and Agrawal, Priyanka and Schroecker, Yannick and Fortin, Alisa and Warmuth, Manfred and Liu, Ji and Leach, Andrew and Blok, Irina and Girirajan, Ganesh Poomal and Aharoni, Roee and Uria, Benigno and Sozanschi, Andrei and Goldberg, Dan and Ionita, Lucian and Ribeiro, Marco Tulio and Zlocha, Martin and Birodkar, Vighnesh and Lachgar, Sami and Yuan, Liangzhe and Choudhury, Himadri and Ginsberg, Matt and Zheng, Fei and Dibb, Gregory and Graves, Emily and Lokhande, Swachhand and Rasskin, Gabriel and Muraru, George-Cristian and Quick, Corbin and Tata, Sandeep and Sermanet, Pierre and Chawla, Aditya and Karo, Itay and Wang, Yan and Zhang, Susan and Keller, Orgad and Dragan, Anca and Su, Guolong and Chou, Ian and Liu, Xi and Tao, Yiqing and Prabhakara, Shruthi and Wilson, Marc and Liu, Ruibo and Wang, Shibo and Evans, Georgie and Du, David and Castaño, Alfonso and Prasad, Gautam and Mahdy, Mona El and Gerlach, Sebastian and Reid, Machel and Kahn, Jarrod and Zait, Amir and Pillai, Thanumalayan Sankaranarayana and Ulrich, Thatcher and Wang, Guanyu and Wassenberg, Jan and Farkash, Efrat and Yalasangi, Kiran and Wang, Congchao and Bauza, Maria and Bucher, Simon and Liu, Ting and Yan, Jun and Leung, Gary and Sindhwani, Vikas and Barnes, Parker and Singh, Avi and Jurin, Ivan and Chang, Jichuan and Bhumihar, Niket Kumar and Eiger, Sivan and Citovsky, Gui and Withbroe, Ben and Li, Zhang and Xue, Siyang and Santo, Niccolò Dal and Stoyanov, Georgi and Raimond, Yves and Zheng, Steven and Gao, Yilin and Listík, Vít and Kwasiborski, Sławek and Saputro, Rachel and Ozturel, Adnan and Mallya, Ganesh and Majmundar, Kushal and West, Ross and Caron, Paul and Wei, Jinliang and Castrejon, Lluis and Vikram, Sharad and Ramachandran, Deepak and Dhawan, Nikhil and Park, Jiho and Smoot, Sara and Driessche, George van den and Blau, Yochai and Malik, Chase and Liang, Wei and Hirsch, Roy and Santos, Cicero Nogueira dos and Weinstein, Eugene and Oord, Aäron van den and Lall, Sid and FitzGerald, Nicholas and Jiang, Zixuan and Yang, Xuan and Webster, Dale and Elqursh, Ali and Pope, Aedan and Rotival, Georges and Raposo, David and Zhu, Wanzheng and Dean, Jeff and Alabed, Sami and Tran, Dustin and Gupta, Arushi and Gleicher, Zach and Austin, Jessica and Rosseel, Edouard and Umekar, Megh and Das, Dipanjan and Sun, Yinghao and Chen, Kai and Misiunas, Karolis and Zhou, Xiang and Di, Yixian and Loo, Alyssa and Newlan, Josh and Li, Bo and Ramasesh, Vinay and Xu, Ying and Chen, Alex and Gandhe, Sudeep and Soricut, Radu and Gupta, Nikita and Hu, Shuguang and El-Sayed, Seliem and Garcia, Xavier and Brusilovsky, Idan and Chen, Pu-Chin and Bolt, Andrew and Huang, Lu and Gurney, Alex and Zhang, Zhiying and Pritzel, Alexander and Wilkiewicz, Jarek and Seybold, Bryan and Shamanna, Bhargav Kanagal and Fischer, Felix and Dean, Josef and Gill, Karan and Mcilroy, Ross and Bhowmick, Abhishek and Selier, Jeremy and Yang, Antoine and Cheng, Derek and Magay, Vladimir and Tan, Jie and Varma, Dhriti and Walder, Christian and Kocisky, Tomas and Nakashima, Ryo and Natsev, Paul and Kwong, Mike and Gog, Ionel and Zhang, Chiyuan and Dieleman, Sander and Jimma, Thomas and Ryabtsev, Andrey and Brahma, Siddhartha and Steiner, David and Du, Dayou and Žužul, Ante and Žanić, Mislav and Raghavachari, Mukund and Gierke, Willi and Zheng, Zeyu and Petrova, Dessie and Dauphin, Yann and Liu, Yuchuan and Kessler, Ido and Hand, Steven and Duvarney, Chris and Kim, Seokhwan and Lee, Hyo and Hussenot, Léonard and Hui, Jeffrey and Smith, Josh and Jain, Deepali and Xia, Jiawei and Tomar, Gaurav Singh and Amiri, Keyvan and Phan, Du and Fuchs, Fabian and Weyand, Tobias and Tomasev, Nenad and Cordell, Alexandra and Liu, Xin and Mallinson, Jonathan and Joshi, Pankaj and Crawford, Andy and Suggala, Arun and Chien, Steve and Fernando, Nick and Sanchez-Vargas, Mariella and Williams, Duncan and Crone, Phil and Luo, Xiyang and Karpov, Igor and Shan, Jyn and Thurk, Terry and Strudel, Robin and Voigtlaender, Paul and Patil, Piyush and Dozat, Tim and Khodaei, Ali and Singla, Sahil and Ambroszczyk, Piotr and Wu, Qiyin and Chang, Yifan and Roark, Brian and Hegde, Chaitra and Ding, Tianli and Filos, Angelos and Wu, Zhongru and Pinto, André Susano and Liu, Shuang and Khanna, Saarthak and Pandey, Aditya and Mcloughlin, Siobhan and Li, Qiujia and Haves, Sam and Zhou, Allan and Buchatskaya, Elena and Leal, Isabel and Boursac, Peter de and Akazawa, Nami and Anderson, Nina and Chen, Terry and Somandepalli, Krishna and Liang, Chen and Goenka, Sheela and Winkler, Stephanie and Grushetsky, Alexander and Ding, Yifan and Smith, Jamie and Ye, Fan and Pont-Tuset, Jordi and Li, Eric and Li, Ruichao and Golany, Tomer and Wegner, Dawid and Jiang, Tao and Barak, Omer and Shangguan, Yuan and Vértes, Eszter and Wong, Renee and Bornschein, Jörg and Tudor, Alex and Bevilacqua, Michele and Schaul, Tom and Rawat, Ankit Singh and Zhao, Yang and Axiotis, Kyriakos and Meng, Lei and McLean, Cory and Lai, Jonathan and Beattie, Jennifer and Kushman, Nate and Liu, Yaxin and Kutzman, Blair and Lang, Fiona and Ye, Jingchen and Netrapalli, Praneeth and Mishra, Pushkar and Khan, Myriam and Goel, Megha and Willoughby, Rob and Tian, David and Zhuang, Honglei and Chen, J. D. and Tsai, Zak and Kementsietsidis, Tasos and Khare, Arjun and Keeling, James and Xu, Keyang and Waters, Nathan and Altché, Florent and Popat, Ashok and Mittal, Bhavishya and Saxton, David and Badawy, Dalia El and Mathieu, Michael and Zheng, Zheng and Zhou, Hao and Ranka, Nishant and Shin, Richard and Duan, Qingnan and Salimans, Tim and Mihailescu, Ioana and Shaham, Uri and Chang, Ming-Wei and Assael, Yannis and Dikkala, Nishanth and Izzard, Martin and Cohen-Addad, Vincent and Graves, Cat and Feinberg, Vlad and Chung, Grace and Strouse, D. J. and Karmon, Danny and Sharifzadeh, Sahand and Ashwood, Zoe and Pham, Khiem and Blanton, Jon and Vasiloff, Alex and Barber, Jarred and Geller, Mark and Zhou, Aurick and Zubach, Fedir and Huang, Tzu-Kuo and Zhang, Lei and Gupta, Himanshu and Young, Matt and Proskurnia, Julia and Votel, Ronny and Gabeur, Valentin and Barcik, Gabriel and Tripathi, Aditya and Yu, Hongkun and Yan, Geng and Changpinyo, Beer and Pavetić, Filip and Coyle, Amy and Fujii, Yasuhisa and Mendez, Jorge Gonzalez and Zhou, Tianhao and Rajamani, Harish and Hechtman, Blake and Cao, Eddie and Juan, Da-Cheng and Tan, Yi-Xuan and Dalibard, Valentin and Du, Yilun and Clay, Natalie and Yao, Kaisheng and Jia, Wenhao and Vijaykumar, Dimple and Zhou, Yuxiang and Bai, Xinyi and Hung, Wei-Chih and Pecht, Steven and Todorov, Georgi and Khadke, Nikhil and Gupta, Pramod and Lahoti, Preethi and Autef, Arnaud and Duddu, Karthik and Lee-Thorp, James and Bykovsky, Alexander and Misiunas, Tautvydas and Flennerhag, Sebastian and Thangaraj, Santhosh and McGiffin, Jed and Nado, Zack and Kunesch, Markus and Noever, Andreas and Hertz, Amir and Liang, Marco and Stone, Victor and Palmer, Evan and Daruki, Samira and Pramanik, Arijit and Põder, Siim and Kyker, Austin and Khan, Mina and Sluzhaev, Evgeny and Ritter, Marvin and Ruderman, Avraham and Zhou, Wenlei and Nagpal, Chirag and Vodrahalli, Kiran and Necula, George and Barham, Paul and Pavlick, Ellie and Hartford, Jay and Shafran, Izhak and Zhao, Long and Mikuła, Maciej and Eccles, Tom and Shimokawa, Hidetoshi and Garg, Kanav and Vilnis, Luke and Chen, Hanwen and Shumailov, Ilia and Lee, Kuang-Huei and Abdelhamed, Abdelrahman and Xie, Meiyan and Cohen, Vered and Hlavnova, Ester and Malkin, Dan and Sitawarin, Chawin and Lottes, James and Coquinot, Pauline and Yu, Tianli and Kumar, Sandeep and Zhang, Jingwei and Mahendru, Aroma and Ahmed, Zafarali and Martens, James and Chen, Tao and Boag, Aviel and Peng, Daiyi and Devin, Coline and Klimovskiy, Arseniy and Phuong, Mary and Vainstein, Danny and Xie, Jin and Ramabhadran, Bhuvana and Howard, Nathan and Yu, Xinxin and Goswami, Gitartha and Cui, Jingyu and Shleifer, Sam and Pinto, Mario and Yeh, Chih-Kuan and Yang, Ming-Hsuan and Javanmardi, Sara and Ethier, Dan and Lee, Chace and Orbay, Jordi and Kotecha, Suyog and Bromberg, Carla and Shaw, Pete and Thornton, James and Rosenthal, Adi Gerzi and Gu, Shane and Thomas, Matt and Gemp, Ian and Ayyar, Aditya and Ushio, Asahi and Selvan, Aarush and Wee, Joel and Liu, Chenxi and Majzoubi, Maryam and Yu, Weiren and Abernethy, Jake and Liechty, Tyler and Pan, Renke and Nguyen, Hoang and Qiong and Hu and Perrin, Sarah and Arora, Abhinav and Pitler, Emily and Wang, Weiyi and Shivakumar, Kaushik and Prost, Flavien and Limonchik, Ben and Wang, Jing and Gao, Yi and Cour, Timothee and Buch, Shyamal and Gui, Huan and Ivanova, Maria and Neubeck, Philipp and Chan, Kelvin and Kim, Lucy and Chen, Huizhong and Goyal, Naman and Chung, Da-Woon and Liu, Lu and Su, Yao and Petrushkina, Anastasia and Shen, Jiajun and Joulin, Armand and Xu, Yuanzhong and Lin, Stein Xudong and Kulizhskaya, Yana and Chelba, Ciprian and Vasudevan, Shobha and Collins, Eli and Bashlovkina, Vasilisa and Lu, Tony and Fritz, Doug and Park, Jongbin and Zhou, Yanqi and Su, Chen and Tanburn, Richard and Sushkov, Mikhail and Rasquinha, Mitchelle and Li, Jinning and Prendki, Jennifer and Li, Yiming and LV, Pallavi and Sharma, Shriya and Fitoussi, Hen and Huang, Hui and Dai, Andrew and Dao, Phuong and Burrows, Mike and Prior, Henry and Qin, Danfeng and Pundak, Golan and Sjoesund, Lars Lowe and Khurshudov, Art and Zhu, Zhenkai and Webson, Albert and Kemp, Elizabeth and Tan, Tat and Agrawal, Saurabh and Sargsyan, Susie and Cheng, Liqun and Stephan, Jim and Kwiatkowski, Tom and Reid, David and Byravan, Arunkumar and Michaely, Assaf Hurwitz and Heess, Nicolas and Zhou, Luowei and Goenka, Sonam and Carpenter, Viral and Levskaya, Anselm and Wang, Bo and Roberts, Reed and Leblond, Rémi and Chikkerur, Sharat and Ginzburg, Stav and Chang, Max and Riachi, Robert and Chuqiao and Xu and Borsos, Zalán and Pliskin, Michael and Pawar, Julia and Lustman, Morgane and Kirkwood, Hannah and Anand, Ankit and Chaudhary, Aditi and Kalb, Norbert and Milan, Kieran and Augenstein, Sean and Goldie, Anna and Prince, Laurel and Raman, Karthik and Sun, Yanhua and Xia, Vivian and Cohen, Aaron and Huo, Zhouyuan and Camp, Josh and Ellis, Seher and Zilka, Lukas and Torres, David Vilar and Patel, Lisa and Arora, Sho and Chan, Betty and Adler, Jonas and Ayoub, Kareem and Liang, Jacky and Jamil, Fayaz and Jiang, Jiepu and Baumgartner, Simon and Sun, Haitian and Karov, Yael and Akulov, Yaroslav and Zheng, Hui and Cai, Irene and Fantacci, Claudio and Rubin, James and Acha, Alex Rav and Wang, Mengchao and D'Souza, Nina and Sathyanarayana, Rohit and Dai, Shengyang and Rowe, Simon and Simanovsky, Andrey and Goldman, Omer and Kuang, Yuheng and Pan, Xiaoyue and Rosenberg, Andrew and Rojas-Esponda, Tania and Dutta, Praneet and Zeng, Amy and Jurenka, Irina and Farquhar, Greg and Bansal, Yamini and Iqbal, Shariq and Roelofs, Becca and Joung, Ga-Young and Beak, Parker and Ryu, Changwan and Poplin, Ryan and Wu, Yan and Alayrac, Jean-Baptiste and Buthpitiya, Senaka and Ronneberger, Olaf and Habtegebriel, Caleb and Li, Wei and Cavallaro, Paul and Wei, Aurora and Bensky, Guy and Denk, Timo and Ganapathy, Harish and Stanway, Jeff and Joshi, Pratik and Bertolini, Francesco and Lo, Jessica and Ma, Olivia and Charles, Zachary and Sampemane, Geta and Sahni, Himanshu and Chen, Xu and Askham, Harry and Gaddy, David and Young, Peter and Tan, Jiewen and Eyal, Matan and Bražinskas, Arthur and Zhong, Li and Wu, Zhichun and Epstein, Mark and Bailey, Kai and Hard, Andrew and Lee, Kamyu and Goldshtein, Sasha and Ruiz, Alex and Badawi, Mohammed and Lochbrunner, Matthias and Kearns, J. K. and Brown, Ashley and Pardo, Fabio and Weber, Theophane and Yang, Haichuan and Jiang, Pan-Pan and Akin, Berkin and Fu, Zhao and Wainwright, Marcus and Zou, Chi and Gaba, Meenu and Manzagol, Pierre-Antoine and Kan, Wendy and Song, Yang and Zainullina, Karina and Lin, Rui and Ko, Jeongwoo and Deshmukh, Salil and Jindal, Apoorv and Svensson, James and Tyam, Divya and Zhao, Heri and Kaeser-Chen, Christine and Baird, Scott and Moradi, Pooya and Hall, Jamie and Guo, Qiuchen and Tsang, Vincent and Liang, Bowen and Pereira, Fernando and Ganesh, Suhas and Korotkov, Ivan and Adamek, Jakub and Thiagarajan, Sridhar and Tran, Vinh and Chen, Charles and Tar, Chris and Jain, Sanil and Dasgupta, Ishita and Bilal, Taylan and Reitter, David and Zhao, Kai and Vezzani, Giulia and Gehman, Yasmin and Mehta, Pulkit and Beltrone, Lauren and Dotiwalla, Xerxes and Guadarrama, Sergio and Abbas, Zaheer and Karp, Stefani and Georgiev, Petko and Ferng, Chun-Sung and Brockschmidt, Marc and Peng, Liqian and Hirnschall, Christoph and Verma, Vikas and Bi, Yingying and Xiao, Ying and Dabush, Avigail and Xu, Kelvin and Wallis, Phil and Parker, Randall and Wang, Qifei and Xu, Yang and Safarli, Ilkin and Tewari, Dinesh and Zhang, Yin and Kim, Seungyeon and Gesmundo, Andrea and Thomas, Mackenzie and Levi, Sergey and Chowdhury, Ahmed and Rao, Kanishka and Garst, Peter and Conway-Rahman, Sam and Ran, Helen and McKinney, Kay and Xiao, Zhisheng and Yu, Wenhao and Agrawal, Rohan and Stjerngren, Axel and Ionescu, Catalin and Chen, Jingjing and Sharma, Vivek and Chiu, Justin and Liu, Fei and Franko, Ken and Sanford, Clayton and Cai, Xingyu and Michel, Paul and Ganapathy, Sanjay and Labanowski, Jane and Garrett, Zachary and Vargas, Ben and Sun, Sean and Gale, Bryan and Buschmann, Thomas and Desjardins, Guillaume and Ghelani, Nimesh and Jain, Palak and Verma, Mudit and Asawaroengchai, Chulayuth and Eisenschlos, Julian and Harlalka, Jitendra and Kazawa, Hideto and Metzler, Don and Howland, Joshua and Jian, Ying and Ades, Jake and Shah, Viral and Gangwani, Tynan and Lee, Seungji and Ring, Roman and Hernandez, Steven M. and Reich, Dean and Sinha, Amer and Sathe, Ashutosh and Kovac, Joe and Gill, Ashleah and Kannan, Ajay and D'olimpio, Andrea and Sevenich, Martin and Whang, Jay and Kim, Been and Sim, Khe Chai and Chen, Jilin and Zhang, Jiageng and Lall, Shuba and Matias, Yossi and Jia, Bill and Friesen, Abe and Nasso, Sara and Thapliyal, Ashish and Perozzi, Bryan and Yu, Ting and Shekhawat, Anna and Huda, Safeen and Grabowski, Peter and Wang, Eric and Sreevatsa, Ashwin and Dib, Hilal and Hassen, Mehadi and Schuh, Parker and Milutinovic, Vedrana and Welty, Chris and Quinn, Michael and Shah, Ali and Wang, Bangju and Barth-Maron, Gabe and Frye, Justin and Axelsson, Natalie and Zhu, Tao and Ma, Yukun and Giannoumis, Irene and Sedghi, Hanie and Ye, Chang and Luan, Yi and Aydin, Kevin and Chandra, Bilva and Sampathkumar, Vivek and Huang, Ronny and Lavrenko, Victor and Eleryan, Ahmed and Hong, Zhi and Hansen, Steven and Carthy, Sara Mc and Samanta, Bidisha and Ćevid, Domagoj and Wang, Xin and Li, Fangtao and Voznesensky, Michael and Hoffman, Matt and Terzis, Andreas and Sehwag, Vikash and Fidel, Gil and He, Luheng and Cai, Mu and He, Yanzhang and Feng, Alex and Nikoltchev, Martin and Phatale, Samrat and Chase, Jason and Lawton, Rory and Zhang, Ming and Ouyang, Tom and Tragut, Manuel and Manshadi, Mehdi Hafezi and Narayanan, Arjun and Shen, Jiaming and Gao, Xu and Bolukbasi, Tolga and Roy, Nick and Li, Xin and Golovin, Daniel and Panait, Liviu and Qin, Zhen and Han, Guangxing and Anthony, Thomas and Kudugunta, Sneha and Patraucean, Viorica and Ray, Aniket and Chen, Xinyun and Yang, Xiaochen and Bhatia, Tanuj and Talluri, Pranav and Morris, Alex and Ražnatović, Andrija and Brownfield, Bethanie and An, James and Peng, Sheng and Kane, Patrick and Zheng, Ce and Duduta, Nico and Kessinger, Joshua and Noraky, James and Liu, Siqi and Rong, Keran and Veličković, Petar and Rush, Keith and Goldin, Alex and Wei, Fanny and Garlapati, Shiva Mohan Reddy and Pantofaru, Caroline and Kwon, Okwan and Ni, Jianmo and Noland, Eric and Trapani, Julia Di and Beaufays, Françoise and Roy, Abhijit Guha and Chow, Yinlam and Turker, Aybuke and Cideron, Geoffrey and Mei, Lantao and Clark, Jon and Dou, Qingyun and Bošnjak, Matko and Leith, Ralph and Du, Yuqing and Yazdanbakhsh, Amir and Nasr, Milad and Kwak, Chester and Sheth, Suraj Satishkumar and Kaskasoli, Alex and Anand, Ankesh and Lakshminarayanan, Balaji and Jerome, Sammy and Bieber, David and Chu, Chun-Te and Senges, Alexandre and Shen, Tianxiao and Sridhar, Mukund and Ndebele, Ndaba and Beyret, Benjamin and Mohamed, Shakir and Chen, Mia and Freitag, Markus and Guo, Jiaxian and Liu, Luyang and Roit, Paul and Chen, Heng and Yan, Shen and Stone, Tom and Co-Reyes, J. D. and Cole, Jeremy and Scellato, Salvatore and Azizi, Shekoofeh and Hashemi, Hadi and Jin, Alicia and Iyer, Anand and Valentine, Marcella and György, András and Ahuja, Arun and Diaz, Daniel Hernandez and Lee, Chen-Yu and Clement, Nathan and Kong, Weize and Garmon, Drew and Watts, Ishaan and Bhatia, Kush and Gupta, Khyatti and Miecnikowski, Matt and Vallet, Hugo and Taly, Ankur and Loper, Edward and Joshi, Saket and Atwood, James and Chick, Jo and Collier, Mark and Iliopoulos, Fotis and Trostle, Ryan and Gunel, Beliz and Leal-Cavazos, Ramiro and Hrafnkelsson, Arnar Mar and Guzman, Michael and Ju, Xiaoen and Forbes, Andy and Emond, Jesse and Chauhan, Kushal and Caine, Ben and Xiao, Li and Zeng, Wenjun and Moufarek, Alexandre and Murphy, Daniel and Meng, Maya and Gupta, Nitish and Riedel, Felix and Das, Anil and Lawal, Elijah and Narayan, Shashi and Sosea, Tiberiu and Swirhun, James and Friso, Linda and Neyshabur, Behnam and Lu, Jing and Girgin, Sertan and Wunder, Michael and Yvinec, Edouard and Pyne, Aroonalok and Carbune, Victor and Rijhwani, Shruti and Guo, Yang and Doshi, Tulsee and Briukhov, Anton and Bain, Max and Hitron, Ayal and Wang, Xuanhui and Gupta, Ashish and Chen, Ke and Du, Cosmo and Zhang, Weiyang and Shah, Dhruv and Akula, Arjun and Dylla, Max and Kachra, Ashyana and Kuo, Weicheng and Zou, Tingting and Wang, Lily and Xu, Luyao and Zhu, Jifan and Snyder, Justin and Menon, Sachit and Firat, Orhan and Mordatch, Igor and Yuan, Yuan and Ponomareva, Natalia and Blevins, Rory and Moore, Lawrence and Wang, Weijun and Chen, Phil and Scholz, Martin and Dwornik, Artur and Lin, Jason and Li, Sicheng and Antognini, Diego and I, Te and Song, Xiaodan and Miller, Matt and Kalra, Uday and Raveret, Adam and Akerlund, Oscar and Wu, Felix and Nystrom, Andrew and Godbole, Namrata and Liu, Tianqi and DeBalsi, Hannah and Zhao, Jewel and Liu, Buhuang and Caciularu, Avi and Lax, Lauren and Khandelwal, Urvashi and Langston, Victoria and Bailey, Eric and Lattanzi, Silvio and Wang, Yufei and Kovelamudi, Neel and Mondal, Sneha and Guruganesh, Guru and Hua, Nan and Roval, Ofir and Wesołowski, Paweł and Ingale, Rishikesh and Halcrow, Jonathan and Sohn, Tim and Angermueller, Christof and Raad, Bahram and Stickgold, Eli and Lu, Eva and Kosik, Alec and Xie, Jing and Lillicrap, Timothy and Huang, Austin and Zhang, Lydia Lihui and Paulus, Dominik and Farabet, Clement and Wertheim, Alex and Wang, Bing and Joshi, Rishabh and Ko, Chu-ling and Wu, Yonghui and Agrawal, Shubham and Lin, Lily and Sheng, XiangHai and Sung, Peter and Breland-King, Tyler and Butterfield, Christina and Gawde, Swapnil and Singh, Sumeet and Zhang, Qiao and Apte, Raj and Shetty, Shilpa and Hutter, Adrian and Li, Tao and Salesky, Elizabeth and Lebron, Federico and Kanerva, Jonni and Paganini, Michela and Nguyen, Arthur and Vallu, Rohith and Peter, Jan-Thorsten and Velury, Sarmishta and Kao, David and Hoover, Jay and Bortsova, Anna and Bishop, Colton and Jakobovits, Shoshana and Agostini, Alessandro and Agarwal, Alekh and Liu, Chang and Kwong, Charles and Tavakkol, Sasan and Bica, Ioana and Greve, Alex and GP, Anirudh and Marcus, Jake and Hou, Le and Duerig, Tom and Moroshko, Rivka and Lacey, Dave and Davis, Andy and Amelot, Julien and Wang, Guohui and Kim, Frank and Strinopoulos, Theofilos and Wan, Hui and Lan, Charline Le and Krishnan, Shankar and Tang, Haotian and Humphreys, Peter and Bai, Junwen and Shtacher, Idan Heimlich and Machado, Diego and Pang, Chenxi and Burke, Ken and Liu, Dangyi and Aravamudhan, Renga and Song, Yue and Hirst, Ed and Singh, Abhimanyu and Jou, Brendan and Bai, Liang and Piccinno, Francesco and Fu, Chuyuan Kelly and Alazard, Robin and Meiri, Barak and Winter, Daniel and Chen, Charlie and Zhang, Mingda and Heitkaemper, Jens and Lambert, John and Lee, Jinhyuk and Frömmgen, Alexander and Rogulenko, Sergey and Nair, Pranav and Niemczyk, Paul and Bulyenov, Anton and Xu, Bibo and Shemtov, Hadar and Zadimoghaddam, Morteza and Toropov, Serge and Wirth, Mateo and Dai, Hanjun and Gollapudi, Sreenivas and Zheng, Daniel and Kurakin, Alex and Lee, Chansoo and Bullard, Kalesha and Serrano, Nicolas and Balazevic, Ivana and Li, Yang and Schalkwyk, Johan and Murphy, Mark and Zhang, Mingyang and Sequeira, Kevin and Datta, Romina and Agrawal, Nishant and Sutton, Charles and Attaluri, Nithya and Chiang, Mencher and Farhan, Wael and Thornton, Gregory and Lin, Kate and Choma, Travis and Nguyen, Hung and Dasgupta, Kingshuk and Robinson, Dirk and Comşa, Iulia and Riley, Michael and Pillai, Arjun and Mustafa, Basil and Golan, Ben and Zandieh, Amir and Lespiau, Jean-Baptiste and Porter, Billy and Ross, David and Rajayogam, Sujeevan and Agarwal, Mohit and Venugopalan, Subhashini and Shahriari, Bobak and Yan, Qiqi and Xu, Hao and Tobin, Taylor and Dubov, Pavel and Shi, Hongzhi and Recasens, Adrià and Kovsharov, Anton and Borgeaud, Sebastian and Dery, Lucio and Vasanth, Shanthal and Gribovskaya, Elena and Qiu, Linhai and Mahdieh, Mahdis and Skut, Wojtek and Nielsen, Elizabeth and Zheng, C. J. and Yu, Adams and Bostock, Carrie Grimes and Gupta, Shaleen and Archer, Aaron and Rawles, Chris and Davies, Elinor and Svyatkovskiy, Alexey and Tsai, Tomy and Halpern, Yoni and Reisswig, Christian and Wydrowski, Bartek and Chang, Bo and Puigcerver, Joan and Taege, Mor Hazan and Li, Jian and Schnider, Eva and Li, Xinjian and Dena, Dragos and Xu, Yunhan and Telang, Umesh and Shi, Tianze and Zen, Heiga and Kastner, Kyle and Ko, Yeongil and Subramaniam, Neesha and Kumar, Aviral and Blois, Pete and Dai, Zhuyun and Wieting, John and Lu, Yifeng and Zeldes, Yoel and Xie, Tian and Hauth, Anja and Ţifrea, Alexandru and Li, Yuqi and El-Husseini, Sam and Abolafia, Dan and Zhou, Howard and Ding, Wen and Ghalebikesabi, Sahra and Guía, Carlos and Maksai, Andrii and Weisz, Ágoston and Arik, Sercan and Sukhanov, Nick and Świetlik, Aga and Jia, Xuhui and Yu, Luo and Wang, Weiyue and Brand, Mark and Bloxwich, Dawn and Kirmani, Sean and Chen, Zhe and Go, Alec and Sprechmann, Pablo and Kannen, Nithish and Carin, Alen and Sandhu, Paramjit and Edkins, Isabel and Nooteboom, Leslie and Gupta, Jai and Maggiore, Loren and Azizi, Javad and Pritch, Yael and Yin, Pengcheng and Gupta, Mansi and Tarlow, Danny and Smith, Duncan and Ivanov, Desi and Babaeizadeh, Mohammad and Goel, Ankita and Kambala, Satish and Chu, Grace and Kastelic, Matej and Liu, Michelle and Soltau, Hagen and Stone, Austin and Agrawal, Shivani and Kim, Min and Soparkar, Kedar and Tadepalli, Srinivas and Bunyan, Oskar and Soh, Rachel and Kannan, Arvind and Kim, D. Y. and Chen, Blake JianHang and Halumi, Afief and Roy, Sudeshna and Wang, Yulong and Sercinoglu, Olcan and Gibson, Gena and Bhatnagar, Sijal and Sano, Motoki and Dincklage, Daniel von and Ren, Qingchun and Mitrevski, Blagoj and Olšák, Mirek and She, Jennifer and Doersch, Carl and Jilei and Wang and Liu, Bingyuan and Tan, Qijun and Yakar, Tamar and Warkentin, Tris and Ramirez, Alex and Lebsack, Carl and Dillon, Josh and Mathews, Rajiv and Cobley, Tom and Wu, Zelin and Chen, Zhuoyuan and Simon, Jon and Nath, Swaroop and Sainath, Tara and Bendebury, Alexei and Julian, Ryan and Mankalale, Bharath and Ćurko, Daria and Zacchello, Paulo and Brown, Adam R. and Sodhia, Kiranbir and Howard, Heidi and Caelles, Sergi and Gupta, Abhinav and Evans, Gareth and Bulanova, Anna and Katzen, Lesley and Goldenberg, Roman and Tsitsulin, Anton and Stanton, Joe and Schillings, Benoit and Kovalev, Vitaly and Fry, Corey and Shah, Rushin and Lin, Kuo and Upadhyay, Shyam and Li, Cheng and Radpour, Soroush and Maggioni, Marcello and Xiong, Jing and Haas, Lukas and Brennan, Jenny and Kamath, Aishwarya and Savinov, Nikolay and Nagrani, Arsha and Yacovone, Trevor and Kappedal, Ryan and Andriopoulos, Kostas and Lao, Li and Li, YaGuang and Rozhdestvenskiy, Grigory and Hashimoto, Kazuma and Audibert, Andrew and Austin, Sophia and Rodriguez, Daniel and Ruoss, Anian and Honke, Garrett and Karkhanis, Deep and Xiong, Xi and Wei, Qing and Huang, James and Leng, Zhaoqi and Premachandran, Vittal and Bileschi, Stan and Evangelopoulos, Georgios and Mensink, Thomas and Pavagadhi, Jay and Teplyashin, Denis and Chang, Paul and Xue, Linting and Tanzer, Garrett and Goldman, Sally and Patel, Kaushal and Li, Shixin and Wiesner, Jeremy and Zheng, Ivy and Stewart-Binks, Ian and Han, Jie and Li, Zhi and Luo, Liangchen and Lenc, Karel and Lučić, Mario and Xue, Fuzhao and Mullins, Ryan and Guseynov, Alexey and Chang, Chung-Ching and Galatzer-Levy, Isaac and Zhang, Adam and Bingham, Garrett and Hu, Grace and Hartman, Ale and Ma, Yue and Griffith, Jordan and Irpan, Alex and Radebaugh, Carey and Yue, Summer and Fan, Lijie and Ungureanu, Victor and Sorokin, Christina and Teufel, Hannah and Li, Peiran and Anil, Rohan and Paparas, Dimitris and Wang, Todd and Lin, Chu-Cheng and Peng, Hui and Shum, Megan and Petrovic, Goran and Brady, Demetra and Nguyen, Richard and Macherey, Klaus and Li, Zhihao and Singh, Harman and Yenugula, Madhavi and Iinuma, Mariko and Chen, Xinyi and Kopparapu, Kavya and Stern, Alexey and Dave, Shachi and Thekkath, Chandu and Perot, Florence and Kumar, Anurag and Li, Fangda and Xiao, Yang and Bilotti, Matthew and Bateni, Mohammad Hossein and Noble, Isaac and Lee, Lisa and Vázquez-Reina, Amelio and Salazar, Julian and Yang, Xiaomeng and Wang, Boyu and Gruzewska, Ela and Rao, Anand and Raghuram, Sindhu and Xu, Zheng and Ben-David, Eyal and Mei, Jieru and Dalmia, Sid and Zhang, Zhaoyi and Liu, Yuchen and Bansal, Gagan and Pankov, Helena and Schwarcz, Steven and Burns, Andrea and Chan, Christine and Sanghai, Sumit and Liang, Ricky and Liang, Ethan and He, Antoine and Stuart, Amy and Narayanan, Arun and Zhu, Yukun and Frank, Christian and Fatemi, Bahar and Sabne, Amit and Lang, Oran and Bhattacharya, Indro and Settle, Shane and Wang, Maria and McMahan, Brendan and Tacchetti, Andrea and Soares, Livio Baldini and Hadian, Majid and Cabi, Serkan and Chung, Timothy and Putikhin, Nikita and Li, Gang and Chen, Jeremy and Tarango, Austin and Michalewski, Henryk and Kazemi, Mehran and Masoom, Hussain and Sheftel, Hila and Shivanna, Rakesh and Vadali, Archita and Comanescu, Ramona and Reid, Doug and Moore, Joss and Neelakantan, Arvind and Sander, Michaël and Herzig, Jonathan and Rosenberg, Aviv and Dehghani, Mostafa and Choi, J. D. and Fink, Michael and Hayes, Reid and Ge, Eric and Weng, Shitao and Ho, Chia-Hua and Karro, John and Krishna, Kalpesh and Thiet, Lam Nguyen and Skerry-Ryan, Amy and Eppens, Daniel and Andreetto, Marco and Sarma, Navin and Bonacina, Silvano and Ayan, Burcu Karagol and Nawhal, Megha and Shan, Zhihao and Dusenberry, Mike and Thakoor, Shantanu and Gubbi, Sagar and Nguyen, Duc Dung and Tsarfaty, Reut and Albanie, Samuel and Mitrović, Jovana and Gandhi, Meet and Chen, Bo-Juen and Epasto, Alessandro and Stephanov, Georgi and Jin, Ye and Gehman, Samuel and Amini, Aida and Weber, Jack and Behbahani, Feryal and Xu, Shawn and Allamanis, Miltos and Chen, Xi and Ott, Myle and Sha, Claire and Jastrzebski, Michal and Qi, Hang and Greene, David and Wu, Xinyi and Toki, Abodunrinwa and Vlasic, Daniel and Shapiro, Jane and Kotikalapudi, Ragha and Shen, Zhe and Saeki, Takaaki and Xie, Sirui and Cassirer, Albin and Bharadwaj, Shikhar and Kiyono, Tatsuya and Bhojanapalli, Srinadh and Rosenfeld, Elan and Ritter, Sam and Mao, Jieming and Oliveira, João Gabriel and Egyed, Zoltan and Bandemer, Bernd and Parisotto, Emilio and Kinoshita, Keisuke and Pluto, Juliette and Maniatis, Petros and Li, Steve and Guo, Yaohui and Ghiasi, Golnaz and Tarbouriech, Jean and Chatterjee, Srimon and Jin, Julie and Katrina and Xu and Palomaki, Jennimaria and Arnold, Séb and Sewak, Madhavi and Piccinini, Federico and Sharma, Mohit and Albrecht, Ben and Purser-haskell, Sean and Vaswani, Ashwin and Chen, Chongyan and Wisniewski, Matheus and Cao, Qin and Aslanides, John and Phu, Nguyet Minh and Sieb, Maximilian and Agubuzu, Lauren and Zheng, Anne and Sohn, Daniel and Selvi, Marco and Andreassen, Anders and Subudhi, Krishan and Eruvbetine, Prem and Woodman, Oliver and Mery, Tomas and Krause, Sebastian and Ren, Xiaoqi and Ma, Xiao and Luo, Jincheng and Chen, Dawn and Fan, Wei and Griffiths, Henry and Schuler, Christian and Li, Alice and Zhang, Shujian and Sarr, Jean-Michel and Luo, Shixin and Patana, Riccardo and Watson, Matthew and Naboulsi, Dani and Collins, Michael and Sidhwani, Sailesh and Hoogeboom, Emiel and Silver, Sharon and Caveness, Emily and Zhao, Xiaokai and Rodriguez, Mikel and Deines, Maxine and Bai, Libin and Griffin, Patrick and Tagliasacchi, Marco and Xue, Emily and Babbula, Spandana Raj and Pang, Bo and Ding, Nan and Shen, Gloria and Peake, Elijah and Crocker, Remi and Raghvendra, Shubha Srinivas and Swisher, Danny and Han, Woohyun and Singh, Richa and Wu, Ling and Pchelin, Vladimir and Munkhdalai, Tsendsuren and Alon, Dana and Bacon, Geoff and Robles, Efren and Bulian, Jannis and Johnson, Melvin and Powell, George and Ferreira, Felipe Tiengo and Li, Yaoyiran and Benzing, Frederik and Velimirović, Mihajlo and Soyer, Hubert and Kong, William and Tony and Nguyên and Yang, Zhen and Liu, Jeremiah and Amersfoort, Joost van and Gillick, Daniel and Sun, Baochen and Rauschmayr, Nathalie and Zhang, Katie and Zhan, Serena and Zhou, Tao and Frolov, Alexey and Yang, Chengrun and Vnukov, Denis and Rouillard, Louis and Li, Hongji and Mandhane, Amol and Fallen, Nova and Venkataraman, Rajesh and Hu, Clara Huiyi and Brennan, Jennifer and Lee, Jenny and Chang, Jerry and Sundermeyer, Martin and Pan, Zhufeng and Ke, Rosemary and Tong, Simon and Fabrikant, Alex and Bono, William and Gu, Jindong and Foley, Ryan and Mao, Yiran and Delakis, Manolis and Bhaswar, Dhruva and Frostig, Roy and Li, Nick and Zipori, Avital and Hope, Cath and Kozlova, Olga and Mishra, Swaroop and Djolonga, Josip and Schiff, Craig and Merey, Majd Al and Briakou, Eleftheria and Morgan, Peter and Wan, Andy and Hassidim, Avinatan and Skerry-Ryan, R. J. and Sengupta, Kuntal and Jasarevic, Mary and Kallakuri, Praveen and Kunkle, Paige and Brennan, Hannah and Lieber, Tom and Mansoor, Hassan and Walker, Julian and Zhang, Bing and Xie, Annie and Žužić, Goran and Chukwuka, Adaeze and Druinsky, Alex and Cho, Donghyun and Yao, Rui and Naeem, Ferjad and Butt, Shiraz and Kim, Eunyoung and Jia, Zhipeng and Jordan, Mandy and Lelkes, Adam and Kurzeja, Mark and Wang, Sophie and Zhao, James and Over, Andrew and Chakladar, Abhishek and Prasetya, Marcel and Jha, Neha and Ganapathy, Sriram and Cong, Yale and Shroff, Prakash and Saroufim, Carl and Miryoosefi, Sobhan and Hammad, Mohamed and Nasir, Tajwar and Xi, Weijuan and Gao, Yang and Maeng, Young and Hora, Ben and Cheng, Chin-Yi and Haghani, Parisa and Lewenberg, Yoad and Lu, Caden and Matysiak, Martin and Raisinghani, Naina and Wang, Huiyu and Baugher, Lexi and Sukthankar, Rahul and Giang, Minh and Schultz, John and Fiedel, Noah and Chen, Minmin and Lee, Cheng-Chun and Dey, Tapomay and Zheng, Hao and Paul, Shachi and Smith, Celine and Ly, Andy and Wang, Yicheng and Bansal, Rishabh and Perz, Bartek and Ricco, Susanna and Blank, Stasha and Keshava, Vaishakh and Sharma, Deepak and Chow, Marvin and Lad, Kunal and Jalan, Komal and Osindero, Simon and Swanson, Craig and Scott, Jacob and Ilić, Anastasija and Li, Xiaowei and Jonnalagadda, Siddhartha Reddy and Soudagar, Afzal Shama and Xiong, Yan and Batsaikhan, Bat-Orgil and Jarrett, Daniel and Kumar, Naveen and Shah, Maulik and Lawlor, Matt and Waters, Austin and Graham, Mark and May, Rhys and Ramos, Sabela and Lefdal, Sandra and Cankara, Zeynep and Cano, Nacho and O'Donoghue, Brendan and Borovik, Jed and Liu, Frederick and Grimstad, Jordan and Alnahlawi, Mahmoud and Tsihlas, Katerina and Hudson, Tom and Grigorev, Nikolai and Jia, Yiling and Huang, Terry and Igwe, Tobenna Peter and Lebedev, Sergei and Tang, Xiaodan and Krivokon, Igor and Garcia, Frankie and Tan, Melissa and Jia, Eric and Stys, Peter and Vashishth, Shikhar and Liang, Yu and Venkatraman, Balaji and Gu, Chenjie and Kementsietsidis, Anastasios and Zhu, Chen and Jung, Junehyuk and Bai, Yunfei and Hosseini, Mohammad Javad and Ahmed, Faruk and Gupta, Aditya and Yuan, Xin and Ashraf, Shereen and Nigam, Shitij and Vasudevan, Gautam and Awasthi, Pranjal and Gilady, Adi Mayrav and Mariet, Zelda and Eskander, Ramy and Li, Haiguang and Hu, Hexiang and Garrido, Guillermo and Schlattner, Philippe and Zhang, George and Saxena, Rohun and Dević, Petar and Muralidharan, Kritika and Murthy, Ashwin and Zhou, Yiqian and Choi, Min and Wongpanich, Arissa and Wang, Zhengdong and Shah, Premal and Xu, Yuntao and Huang, Yiling and Spencer, Stephen and Chen, Alice and Cohan, James and Wang, Junjie and Tompson, Jonathan and Wu, Junru and Haroun, Ruba and Li, Haiqiong and Huergo, Blanca and Yang, Fan and Yin, Tongxin and Wendt, James and Bendersky, Michael and Chaabouni, Rahma and Snaider, Javier and Ferret, Johan and Jindal, Abhishek and Thompson, Tara and Xue, Andrew and Bishop, Will and Phal, Shubham Milind and Sharma, Archit and Sung, Yunhsuan and Radhakrishnan, Prabakar and Shomrat, Mo and Ingle, Reeve and Vij, Roopali and Gilmer, Justin and Istin, Mihai Dorin and Sobell, Sam and Lu, Yang and Nottage, Emily and Sadigh, Dorsa and Willcock, Jeremiah and Zhang, Tingnan and Xu, Steve and Brown, Sasha and Lee, Katherine and Wang, Gary and Zhu, Yun and Tay, Yi and Kim, Cheolmin and Gutierrez, Audrey and Sharma, Abhanshu and Xian, Yongqin and Seo, Sungyong and Cui, Claire and Pochernina, Elena and Baetu, Cip and Jastrzębski, Krzysztof and Ly, Mimi and Elhawaty, Mohamed and Suh, Dan and Sezener, Eren and Wang, Pidong and Yuen, Nancy and Tucker, George and Cai, Jiahao and Yang, Zuguang and Wang, Cindy and Muzio, Alex and Qian, Hai and Yoo, Jae and Lockhart, Derek and McKee, Kevin R. and Guo, Mandy and Mehrotra, Malika and Mendonça, Artur and Mehta, Sanket Vaibhav and Ben, Sherry and Tekur, Chetan and Mu, Jiaqi and Zhu, Muye and Krakovna, Victoria and Lee, Hongrae and Maschinot, A. J. and Cevey, Sébastien and Choe, HyunJeong and Bai, Aijun and Srinivasan, Hansa and Gasaway, Derek and Young, Nick and Siegler, Patrick and Holtmann-Rice, Dan and Piratla, Vihari and Baumli, Kate and Yogev, Roey and Hofer, Alex and Hasselt, Hado van and Grant, Svetlana and Chervonyi, Yuri and Silver, David and Hogue, Andrew and Agarwal, Ayushi and Wang, Kathie and Singh, Preeti and Flynn, Four and Lipschultz, Josh and David, Robert and Bellot, Lizzetth and Yang, Yao-Yuan and Le, Long and Graziano, Filippo and Olszewska, Kate and Hui, Kevin and Maurya, Akanksha and Parotsidis, Nikos and Chen, Weijie and Oguntebi, Tayo and Kelley, Joe and Baddepudi, Anirudh and Mauerer, Johannes and Shaw, Gregory and Siegman, Alex and Yang, Lin and Shetty, Shravya and Roy, Subhrajit and Song, Yunting and Stokowiec, Wojciech and Burnell, Ryan and Savant, Omkar and Busa-Fekete, Robert and Miao, Jin and Ghosh, Samrat and MacDermed, Liam and Lippe, Phillip and Dektiarev, Mikhail and Behrman, Zach and Mentzer, Fabian and Nguyen, Kelvin and Wei, Meng and Verma, Siddharth and Knutsen, Chris and Dasari, Sudeep and Yan, Zhipeng and Mitrichev, Petr and Wang, Xingyu and Shejwalkar, Virat and Austin, Jacob and Sunkara, Srinivas and Potti, Navneet and Virin, Yan and Wright, Christian and Liu, Gaël and Riva, Oriana and Pot, Etienne and Kochanski, Greg and Le, Quoc and Balasubramaniam, Gargi and Dhar, Arka and Liao, Yuguo and Bloniarz, Adam and Shukla, Divyansh and Cole, Elizabeth and Lee, Jong and Zhang, Sheng and Kafle, Sushant and Vashishtha, Siddharth and Mahmoudieh, Parsa and Chen, Grace and Hoffmann, Raphael and Srinivasan, Pranesh and Lago, Agustin Dal and Shalom, Yoav Ben and Wang, Zi and Elabd, Michael and Sharma, Anuj and Oh, Junhyuk and Kothawade, Suraj and Le, Maigo and Monteiro, Marianne and Yang, Shentao and Alarakyia, Kaiz and Geirhos, Robert and Mincu, Diana and Garnes, Håvard and Kobayashi, Hayato and Mariooryad, Soroosh and Krasowiak, Kacper and Zhixin and Lai and Mourad, Shibl and Wang, Mingqiu and Bu, Fan and Aharoni, Ophir and Chen, Guanjie and Goyal, Abhimanyu and Zubov, Vadim and Bapna, Ankur and Dabir, Elahe and Kothari, Nisarg and Lamerigts, Kay and Cao, Nicola De and Shar, Jeremy and Yew, Christopher and Kulkarni, Nitish and Mahaarachchi, Dre and Joshi, Mandar and Zhu, Zhenhai and Lichtarge, Jared and Zhou, Yichao and Muckenhirn, Hannah and Selo, Vittorio and Vinyals, Oriol and Chen, Peter and Brohan, Anthony and Mehta, Vaibhav and Cogan, Sarah and Wang, Ruth and Geri, Ty and Ko, Wei-Jen and Chen, Wei and Viola, Fabio and Shivam, Keshav and Wang, Lisa and Elish, Madeleine Clare and Popa, Raluca Ada and Pereira, Sébastien and Liu, Jianqiao and Koster, Raphael and Kim, Donnie and Zhang, Gufeng and Ebrahimi, Sayna and Talukdar, Partha and Zheng, Yanyan and Poklukar, Petra and Mikhalap, Ales and Johnson, Dale and Vijayakumar, Anitha and Omernick, Mark and Dibb, Matt and Dubey, Ayush and Hu, Qiong and Suman, Apurv and Aggarwal, Vaibhav and Kornakov, Ilya and Xia, Fei and Lowe, Wing and Kolganov, Alexey and Xiao, Ted and Nikolaev, Vitaly and Hemingray, Steven and Li, Bonnie and Iljazi, Joana and Rybiński, Mikołaj and Sandhu, Ballie and Lu, Peggy and Luong, Thang and Jenatton, Rodolphe and Govindaraj, Vineetha and Hui and Li and Dulac-Arnold, Gabriel and Park, Wonpyo and Wang, Henry and Modi, Abhinit and Pouget-Abadie, Jean and Greller, Kristina and Gupta, Rahul and Berry, Robert and Ramachandran, Prajit and Xie, Jinyu and McCafferty, Liam and Wang, Jianling and Gupta, Kilol and Lim, Hyeontaek and Bratanič, Blaž and Brock, Andy and Akolzin, Ilia and Sproch, Jim and Karliner, Dan and Kim, Duhyeon and Goedeckemeyer, Adrian and Shazeer, Noam and Schmid, Cordelia and Calandriello, Daniele and Bhatia, Parul and Choromanski, Krzysztof and Montgomery, Ceslee and Dua, Dheeru and Ramalho, Ana and King, Helen and Gao, Yue and Nguyen, Lynn and Lindner, David and Pitta, Divya and Johnson, Oleaser and Salama, Khalid and Ardila, Diego and Han, Michael and Farnese, Erin and Odoom, Seth and Wang, Ziyue and Ding, Xiangzhuo and Rink, Norman and Smith, Ray and Lehri, Harshal Tushar and Cohen, Eden and Vats, Neera and He, Tong and Gopavarapu, Parthasarathy and Paszke, Adam and Patel, Miteyan and Gansbeke, Wouter Van and Loher, Lucia and Castro, Luis and Voitovich, Maria and Glehn, Tamara von and George, Nelson and Niklaus, Simon and Eaton-Rosen, Zach and Rakićević, Nemanja and Jue, Erik and Perel, Sagi and Zhang, Carrie and Bahat, Yuval and Pouget, Angéline and Xing, Zhi and Huot, Fantine and Shenoy, Ashish and Bos, Taylor and Coriou, Vincent and Richter, Bryan and Noy, Natasha and Wang, Yaqing and Ontanon, Santiago and Qin, Siyang and Makarchuk, Gleb and Hassabis, Demis and Li, Zhuowan and Sharma, Mandar and Venkatesan, Kumaran and Kemaev, Iurii and Daniel, Roxanne and Huang, Shiyu and Shah, Saloni and Ponce, Octavio and Warren and Chen and Faruqui, Manaal and Wu, Jialin and Andačić, Slavica and Payrits, Szabolcs and McDuff, Daniel and Hume, Tom and Cao, Yuan and Tessler, M. H. and Wang, Qingze and Wang, Yinan and Rendulic, Ivor and Agustsson, Eirikur and Johnson, Matthew and Lando, Tanya and Howard, Andrew and Padmanabhan, Sri Gayatri Sundara and Daswani, Mayank and Banino, Andrea and Kilgore, Michael and Heek, Jonathan and Ji, Ziwei and Caceres, Alvaro and Li, Conglong and Kassner, Nora and Vlaskin, Alexey and Liu, Zeyu and Grills, Alex and Hou, Yanhan and Sukkerd, Roykrong and Cheon, Gowoon and Shetty, Nishita and Markeeva, Larisa and Stanczyk, Piotr and Iyer, Tejas and Gong, Yuan and Gao, Shawn and Gopalakrishnan, Keerthana and Blyth, Tim and Reynolds, Malcolm and Bhoopchand, Avishkar and Bilenko, Misha and Gharibian, Dero and Zayats, Vicky and Faust, Aleksandra and Singh, Abhinav and Ma, Min and Jiao, Hongyang and Vijayanarasimhan, Sudheendra and Aroyo, Lora and Yadav, Vikas and Chakera, Sarah and Kakarla, Ashwin and Meshram, Vilobh and Gregor, Karol and Botea, Gabriela and Senter, Evan and Jia, Dawei and Kovacs, Geza and Sharma, Neha and Baur, Sebastien and Kang, Kai and He, Yifan and Zhuo, Lin and Kostelac, Marija and Laish, Itay and Peng, Songyou and O'Bryan, Louis and Kasenberg, Daniel and Rao, Girish Ramchandra and Leurent, Edouard and Zhang, Biao and Stevens, Sage and Salazar, Ana and Zhang, Ye and Lobov, Ivan and Walker, Jake and Porter, Allen and Redshaw, Morgan and Ke, Han and Rao, Abhishek and Lee, Alex and Lam, Hoi and Moffitt, Michael and Kim, Jaeyoun and Qiao, Siyuan and Koo, Terry and Dadashi, Robert and Song, Xinying and Sundararajan, Mukund and Xu, Peng and Kawamoto, Chizu and Zhong, Yan and Barbu, Clara and Reddy, Apoorv and Verzetti, Mauro and Li, Leon and Papamakarios, George and Klimczak-Plucińska, Hanna and Cassin, Mary and Kavukcuoglu, Koray and Swavely, Rigel and Vaucher, Alain and Zhao, Jeffrey and Hemsley, Ross and Tschannen, Michael and Ge, Heming and Menghani, Gaurav and Yu, Yang and Ha, Natalie and He, Wei and Wu, Xiao and Song, Maggie and Sterneck, Rachel and Zinke, Stefan and Calian, Dan A. and Marsden, Annie and Ruiz, Alejandro Cruzado and Hessel, Matteo and Gueta, Almog and Lee, Benjamin and Farris, Brian and Gupta, Manish and Li, Yunjie and Saleh, Mohammad and Misra, Vedant and Xiao, Kefan and Mendolicchio, Piermaria and Buttimore, Gavin and Krayvanova, Varvara and Nayakanti, Nigamaa and Wiethoff, Matthew and Pande, Yash and Mirhoseini, Azalia and Lao, Ni and Liu, Jasmine and Hua, Yiqing and Chen, Angie and Malkov, Yury and Kalashnikov, Dmitry and Gupta, Shubham and Audhkhasi, Kartik and Zhai, Yuexiang and Kopalle, Sudhindra and Jain, Prateek and Ofek, Eran and Meyer, Clemens and Baatarsukh, Khuslen and Strejček, Hana and Qian, Jun and Freedman, James and Figueira, Ricardo and Sokolik, Michal and Bachem, Olivier and Lin, Raymond and Kharrat, Dia and Hidey, Chris and Xu, Pingmei and Duan, Dennis and Li, Yin and Ersoy, Muge and Everett, Richard and Cen, Kevin and Santamaria-Fernandez, Rebeca and Taubenfeld, Amir and Mackinnon, Ian and Deng, Linda and Zablotskaia, Polina and Viswanadha, Shashank and Goel, Shivanker and Yates, Damion and Deng, Yunxiao and Choy, Peter and Chen, Mingqing and Sinha, Abhishek and Mossin, Alex and Wang, Yiming and Szlam, Arthur and Hao, Susan and Rubenstein, Paul Kishan and Toksoz-Exley, Metin and Aperghis, Miranda and Zhong, Yin and Ahn, Junwhan and Isard, Michael and Lacombe, Olivier and Luisier, Florian and Anastasiou, Chrysovalantis and Kalley, Yogesh and Prabhu, Utsav and Dunleavy, Emma and Bijwadia, Shaan and Mao-Jones, Justin and Chen, Kelly and Pasumarthi, Rama and Wood, Emily and Dostmohamed, Adil and Hurley, Nate and Simsa, Jiri and Parrish, Alicia and Pajarskas, Mantas and Harvey, Matt and Skopek, Ondrej and Kochinski, Yony and Rey, Javier and Rieser, Verena and Zhou, Denny and Lee, Sun Jae and Acharya, Trilok and Li, Guowang and Jiang, Joe and Zhang, Xiaofan and Gipson, Bryant and Mahintorabi, Ethan and Gelmi, Marco and Khajehnouri, Nima and Yeh, Angel and Lee, Kayi and Matthey, Loic and Baker, Leslie and Pham, Trang and Fu, Han and Pak, Alex and Gupta, Prakhar and Vasconcelos, Cristina and Sadovsky, Adam and Walker, Brian and Hsiao, Sissie and Zochbauer, Patrik and Marzoca, Andreea and Velan, Noam and Zeng, Junhao and Baechler, Gilles and Driess, Danny and Jain, Divya and Huang, Yanping and Tao, Lizzie and Maggs, John and Levine, Nir and Schneider, Jon and Gemzer, Erika and Petit, Samuel and Han, Shan and Fisher, Zach and Zelle, Dustin and Biles, Courtney and Ie, Eugene and Fadeeva, Asya and Liu, Casper and Franco, Juliana Vicente and Collister, Adrian and Zhang, Hao and Wang, Renshen and Zhao, Ruizhe and Kieliger, Leandro and Shuster, Kurt and Zhu, Rui and Gong, Boqing and Chan, Lawrence and Sun, Ruoxi and Basu, Sujoy and Zimmermann, Roland and Hayes, Jamie and Bapna, Abhishek and Snoek, Jasper and Yang, Weel and Datta, Puranjay and Abdallah, Jad Al and Kilgour, Kevin and Li, Lu and Mah, S. Q. and Jun, Yennie and Rivière, Morgane and Karmarkar, Abhijit and Spalink, Tammo and Huang, Tao and Gonzalez, Lucas and Tran, Duc-Hieu and Nowak, Averi and Palowitch, John and Chadwick, Martin and Talius, Ellie and Mehta, Harsh and Sellam, Thibault and Fränken, Philipp and Nicosia, Massimo and He, Kyle and Kini, Aditya and Amos, David and Basu, Sugato and Jobe, Harrison and Shaw, Eleni and Xu, Qiantong and Evans, Colin and Ikeda, Daisuke and Yan, Chaochao and Jin, Larry and Wang, Lun and Yadav, Sachin and Labzovsky, Ilia and Sampath, Ramesh and Ma, Ada and Schumann, Candice and Siddhant, Aditya and Shah, Rohin and Youssef, John and Agarwal, Rishabh and Dabney, Natalie and Tonioni, Alessio and Ambar, Moran and Li, Jing and Guyon, Isabelle and Li, Benny and Soergel, David and Fang, Boya and Karadzhov, Georgi and Udrescu, Cristian and Trinh, Trieu and Raunak, Vikas and Noury, Seb and Guo, Dee and Gupta, Sonal and Finkelstein, Mara and Petek, Denis and Liang, Lihao and Billock, Greg and Sun, Pei and Wood, David and Song, Yiwen and Yu, Xiaobin and Matejovicova, Tatiana and Cohen, Regev and Andra, Kalyan and D'Ambrosio, David and Deng, Zhiwei and Nallatamby, Vincent and Songhori, Ebrahim and Dangovski, Rumen and Lampinen, Andrew and Botadra, Pankil and Hillier, Adam and Cao, Jiawei and Baddi, Nagabhushan and Kuncoro, Adhi and Yoshino, Toshihiro and Bhagatwala, Ankit and Ranzato, Marcáurelio and Schaeffer, Rylan and Liu, Tianlin and Ye, Shuai and Sarvana, Obaid and Nham, John and Kuang, Chenkai and Gao, Isabel and Baek, Jinoo and Mittal, Shubham and Wahid, Ayzaan and Gergely, Anita and Ni, Bin and Feldman, Josh and Muir, Carrie and Lamblin, Pascal and Macherey, Wolfgang and Dyer, Ethan and Kilpatrick, Logan and Campos, Víctor and Bhutani, Mukul and Fort, Stanislav and Ahmad, Yanif and Severyn, Aliaksei and Chatziprimou, Kleopatra and Ferludin, Oleksandr and Dimarco, Mason and Kusupati, Aditya and Heyward, Joe and Bahir, Dan and Villela, Kevin and Millican, Katie and Marcus, Dror and Bahargam, Sanaz and Unlu, Caglar and Roth, Nicholas and Wei, Zichuan and Gopal, Siddharth and Ghoshal, Deepanway and Lee, Edward and Lin, Sharon and Lees, Jennie and Lee, Dayeong and Hosseini, Anahita and Fan, Connie and Neel, Seth and Wu, Marcus and Altun, Yasemin and Cai, Honglong and Piqueras, Enrique and Woodward, Josh and Bissacco, Alessandro and Haykal, Salem and Bordbar, Mahyar and Sundaram, Prasha and Hodkinson, Sarah and Toyama, Daniel and Polovets, George and Myers, Austin and Sinha, Anu and Levinboim, Tomer and Krishnakumar, Kashyap and Chhaparia, Rachita and Sholokhova, Tatiana and Gundavarapu, Nitesh Bharadwaj and Jawahar, Ganesh and Qureshi, Haroon and Hu, Jieru and Momchev, Nikola and Rahtz, Matthew and Wu, Renjie and S, Aishwarya P. and Dhamdhere, Kedar and Guo, Meiqi and Gupta, Umang and Eslami, Ali and Schain, Mariano and Blokzijl, Michiel and Welling, David and Orr, Dave and Bolelli, Levent and Perez-Nieves, Nicolas and Sirotenko, Mikhail and Prasad, Aman and Kar, Arjun and Pigem, Borja De Balle and Terzi, Tayfun and Weisz, Gellért and Ghosh, Dipankar and Mavalankar, Aditi and Madeka, Dhruv and Daugaard, Kaspar and Adam, Hartwig and Shah, Viraj and Berman, Dana and Tran, Maggie and Baker, Steven and Andrejczuk, Ewa and Chole, Grishma and Raboshchuk, Ganna and Mirzazadeh, Mahdi and Kagohara, Thais and Wu, Shimu and Schallhart, Christian and Orlando, Bernett and Wang, Chen and Rrustemi, Alban and Xiong, Hao and Liu, Hao and Vezer, Arpi and Ramsden, Nolan and Chang, Shuo-yiin and Mudgal, Sidharth and Li, Yan and Vieillard, Nino and Hoshen, Yedid and Ahmad, Farooq and Slone, Ambrose and Hua, Amy and Potikha, Natan and Rossini, Mirko and Stritar, Jon and Prakash, Sushant and Wang, Zifeng and Dong, Xuanyi and Nazari, Alireza and Nehoran, Efrat and Tekelioglu, Kaan and Li, Yinxiao and Badola, Kartikeya and Funkhouser, Tom and Li, Yuanzhen and Yerram, Varun and Ganeshan, Ramya and Formoso, Daniel and Langner, Karol and Shi, Tian and Li, Huijian and Yamamori, Yumeya and Panda, Amayika and Saade, Alaa and Scarpati, Angelo Scorza and Breaux, Chris and Carey, C. J. and Zhou, Zongwei and Hsieh, Cho-Jui and Bridgers, Sophie and Butryna, Alena and Gupta, Nishesh and Tulsyan, Vaibhav and Woo, Sanghyun and Eltyshev, Evgenii and Grathwohl, Will and Parks, Chanel and Benjamin, Seth and Panigrahy, Rina and Dodhia, Shenil and Freitas, Daniel De and Sauer, Chris and Song, Will and Alet, Ferran and Tolins, Jackson and Paduraru, Cosmin and Zhou, Xingyi and Albert, Brian and Zhang, Zizhao and Shu, Lei and Bansal, Mudit and Nguyen, Sarah and Globerson, Amir and Xiao, Owen and Manyika, James and Hennigan, Tom and Rong, Rong and Matak, Josip and Bakalov, Anton and Sharma, Ankur and Sinopalnikov, Danila and Pierson, Andrew and Roller, Stephen and Brown, Geoff and Gao, Mingcen and Fukuzawa, Toshiyuki and Ghafouri, Amin and Vassigh, Kenny and Barr, Iain and Wang, Zhicheng and Korsun, Anna and Jayaram, Rajesh and Ren, Lijie and Zaman, Tim and Khan, Samira and Lunts, Yana and Deutsch, Dan and Uthus, Dave and Katz, Nitzan and Samsikova, Masha and Khalifa, Amr and Sethi, Nikhil and Sun, Jiao and Tang, Luming and Alon, Uri and Luo, Xianghong and Yu, Dian and Nayyar, Abhishek and Petrini, Bryce and Truong, Will and Hellendoorn, Vincent and Chinaev, Nikolai and Alberti, Chris and Wang, Wei and Hu, Jingcao and Mirrokni, Vahab and Balashankar, Ananth and Aharon, Avia and Mehta, Aahil and Iscen, Ahmet and Kready, Joseph and Manning, Lucas and Mohananey, Anhad and Chen, Yuankai and Tripathi, Anshuman and Wu, Allen and Petrovski, Igor and Hwang, Dawsen and Baeuml, Martin and Chandrakaladharan, Shreyas and Liu, Yuan and Coaguila, Rey and Chen, Maxwell and Ma, Sally and Tafti, Pouya and Tatineni, Susheel and Spitz, Terry and Ye, Jiayu and Vicol, Paul and Rosca, Mihaela and Puigdomènech, Adrià and Yahav, Zohar and Ghemawat, Sanjay and Lin, Hanzhao and Kirk, Phoebe and Nabulsi, Zaid and Brin, Sergey and Bohnet, Bernd and Caluwaerts, Ken and Veerubhotla, Aditya Srikanth and Zheng, Dan and Dai, Zihang and Petrov, Petre and Xu, Yichong and Mehran, Ramin and Xu, Zhuo and Zintgraf, Luisa and Choi, Jiho and Hombaiah, Spurthi Amba and Thoppilan, Romal and Reddi, Sashank and Lew, Lukasz and Li, Li and Webster, Kellie and Sawhney, K. P. and Lamprou, Lampros and Shakeri, Siamak and Lunayach, Mayank and Chen, Jianmin and Bagri, Sumit and Salcianu, Alex and Chen, Ying and Donchev, Yani and Magister, Charlotte and Nørly, Signe and Rodrigues, Vitor and Izo, Tomas and Noga, Hila and Zou, Joe and Köppe, Thomas and Zhou, Wenxuan and Lee, Kenton and Long, Xiangzhu and Eisenbud, Danielle and Chen, Anthony and Schenck, Connor and To, Chi Ming and Zhong, Peilin and Taropa, Emanuel and Truong, Minh and Levy, Omer and Martins, Danilo and Zhang, Zhiyuan and Semturs, Christopher and Zhang, Kelvin and Yakubovich, Alex and Moreno, Pol and McConnaughey, Lara and Lu, Di and Redmond, Sam and Weerts, Lotte and Bitton, Yonatan and Refice, Tiziana and Lacasse, Nicolas and Conmy, Arthur and Tallec, Corentin and Odell, Julian and Forbes-Pollard, Hannah and Socala, Arkadiusz and Hoech, Jonathan and Kohli, Pushmeet and Walton, Alanna and Wang, Rui and Sazanovich, Mikita and Zhu, Kexin and Kapishnikov, Andrei and Galt, Rich and Denton, Matthew and Murdoch, Ben and Sikora, Caitlin and Mohamed, Kareem and Wei, Wei and First, Uri and McConnell, Tim and Cobo, Luis C. and Qin, James and Avrahami, Thi and Balle, Daniel and Watanabe, Yu and Louis, Annie and Kraft, Adam and Ariafar, Setareh and Gu, Yiming and Rives, Eugénie and Yoon, Charles and Rusu, Andrei and Cobon-Kerr, James and Hahn, Chris and Luo, Jiaming and Yuvein and Zhu and Ahuja, Niharika and Benenson, Rodrigo and Kaufman, Raphaël Lopez and Yu, Honglin and Hightower, Lloyd and Zhang, Junlin and Ni, Darren and Hendricks, Lisa Anne and Wang, Gabby and Yona, Gal and Jain, Lalit and Barrio, Pablo and Bhupatiraju, Surya and Velusamy, Siva and Dafoe, Allan and Riedel, Sebastian and Thomas, Tara and Yuan, Zhe and Bellaiche, Mathias and Panthaplackel, Sheena and Kloboves, Klemen and Jauhari, Sarthak and Akbulut, Canfer and Davchev, Todor and Gladchenko, Evgeny and Madras, David and Chuklin, Aleksandr and Hill, Tyrone and Yuan, Quan and Madhavan, Mukundan and Leonhard, Luke and Scandinaro, Dylan and Chen, Qihang and Niu, Ning and Douillard, Arthur and Damoc, Bogdan and Onoe, Yasumasa and Pedregosa, Fabian and Bertsch, Fred and Leichner, Chas and Pagadora, Joseph and Malmaud, Jonathan and Ponda, Sameera and Twigg, Andy and Duzhyi, Oleksii and Shen, Jingwei and Wang, Miaosen and Garg, Roopal and Chen, Jing and Evci, Utku and Lee, Jonathan and Liu, Leon and Kojima, Koji and Yamaguchi, Masa and Rajendran, Arunkumar and Piergiovanni, A. J. and Rajendran, Vinodh Kumar and Fornoni, Marco and Ibagon, Gabriel and Ragan, Harry and Khan, Sadh MNM and Blitzer, John and Bunner, Andrew and Sun, Guan and Kosakai, Takahiro and Lundberg, Scott and Elue, Ndidi and Guu, Kelvin and Park, S. K. and Park, Jane and Narayanaswamy, Arunachalam and Wu, Chengda and Mudigonda, Jayaram and Cohn, Trevor and Mu, Hairong and Kumar, Ravi and Graesser, Laura and Zhang, Yichi and Killam, Richard and Zhuang, Vincent and Giménez, Mai and Jishi, Wael Al and Ley-Wild, Ruy and Zhai, Alex and Osawa, Kazuki and Cedillo, Diego and Liu, Jialu and Upadhyay, Mayank and Sieniek, Marcin and Sharma, Roshan and Paine, Tom and Angelova, Anelia and Addepalli, Sravanti and Parada, Carolina and Majumder, Kingshuk and Lamp, Avery and Kumar, Sanjiv and Deng, Xiang and Myaskovsky, Artiom and Sabolić, Tea and Dudek, Jeffrey and York, Sarah and Quitry, Félix de Chaumont and Nie, Jiazhong and Cattle, Dee and Gunjan, Alok and Piot, Bilal and Khawaja, Waleed and Bang, Seojin and Wang, Simon and Khodadadeh, Siavash and R, Raghavender and Rawlani, Praynaa and Powell, Richard and Lee, Kevin and Griesser, Johannes and Oh, G. S. and Magalhaes, Cesar and Li, Yujia and Tokumine, Simon and Vogel, Hadas Natalie and Hsu, Dennis and BC, Arturo and Jindal, Disha and Cohen, Matan and Yang, Zi and Yuan, Junwei and Cesare, Dario de and Bruguier, Tony and Xu, Jun and Roy, Monica and Jacovi, Alon and Belov, Dan and Arya, Rahul and Meadowlark, Phoenix and Cohen-Ganor, Shlomi and Ye, Wenting and Morris-Suzuki, Patrick and Banzal, Praseem and Song, Gan and Ponnuramu, Pranavaraj and Zhang, Fred and Scrivener, George and Zaiem, Salah and Rochman, Alif Raditya and Han, Kehang and Ghazi, Badih and Lee, Kate and Drath, Shahar and Suo, Daniel and Girgis, Antonious and Shenoy, Pradeep and Nguyen, Duy and Eck, Douglas and Gupta, Somit and Yan, Le and Carreira, Joao and Gulati, Anmol and Sang, Ruoxin and Mirylenka, Daniil and Cooney, Emma and Chou, Edward and Ling, Mingyang and Fan, Cindy and Coleman, Ben and Tubone, Guilherme and Kumar, Ravin and Baldridge, Jason and Hernandez-Campos, Felix and Lazaridou, Angeliki and Besley, James and Yona, Itay and Bulut, Neslihan and Wellens, Quentin and Pierigiovanni, A. J. and George, Jasmine and Green, Richard and Han, Pu and Tao, Connie and Clark, Geoff and You, Chong and Abdolmaleki, Abbas and Fu, Justin and Chen, Tongzhou and Chaugule, Ashwin and Chandorkar, Angad and Rahman, Altaf and Thompson, Will and Koanantakool, Penporn and Bernico, Mike and Ren, Jie and Vlasov, Andrey and Vassilvitskii, Sergei and Kula, Maciej and Liang, Yizhong and Kim, Dahun and Huang, Yangsibo and Ye, Chengxi and Lepikhin, Dmitry and Helmholz, Wesley},
	month = oct,
	year = {2025},
	note = {arXiv:2507.06261 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 72 pages, 17 figures},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/MAGHGW9C/Comanici et al. - 2025 - Gemini 2.5 Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Gene.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/JAQQQU9N/2507.html:text/html},
}

@misc{zheng_judging_2023,
	title = {Judging {LLM}-as-a-{Judge} with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	doi = {10.48550/arXiv.2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	urldate = {2025-11-17},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = dec,
	year = {2023},
	note = {arXiv:2306.05685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: NeurIPS 2023 Datasets and Benchmarks Track},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/WFTHM9G7/Zheng et al. - 2023 - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/XG6888FW/2306.html:text/html},
}

@misc{liu_g-eval_2023,
	title = {G-{Eval}: {NLG} {Evaluation} using {GPT}-4 with {Better} {Human} {Alignment}},
	shorttitle = {G-{Eval}},
	url = {http://arxiv.org/abs/2303.16634},
	doi = {10.48550/arXiv.2303.16634},
	abstract = {The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval},
	urldate = {2025-11-17},
	publisher = {arXiv},
	author = {Liu, Yang and Iter, Dan and Xu, Yichong and Wang, Shuohang and Xu, Ruochen and Zhu, Chenguang},
	month = may,
	year = {2023},
	note = {arXiv:2303.16634 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/HT6UWKT3/Liu et al. - 2023 - G-Eval NLG Evaluation using GPT-4 with Better Human Alignment.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/T44TJAJC/2303.html:text/html},
}

@misc{li_evaluating_2025,
	title = {Evaluating {Scoring} {Bias} in {LLM}-as-a-{Judge}},
	url = {http://arxiv.org/abs/2506.22316},
	doi = {10.48550/arXiv.2506.22316},
	abstract = {The remarkable performance of Large Language Models (LLMs) gives rise to``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks. Moreover, it has been widely adopted across fields such as Natural Language Processing (NLP), preference learning, and various specific domains. However, there are various biases within LLM-as-a-Judge, which adversely affect the fairness and reliability of judgments. Current research on evaluating or mitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based evaluations, while systematic investigations into bias in scoring-based evaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge as the scores differ when scoring judge models are bias-related perturbed, and provide a well-designed framework to comprehensively evaluate scoring bias. We augment existing LLM-as-a-Judge benchmarks through data synthesis to construct our evaluation dataset and design multi-faceted evaluation metrics. Our experimental results demonstrate that the scoring stability of existing judge models is disrupted by scoring biases. Further exploratory experiments and discussions provide valuable insights into the design of scoring prompt templates and the mitigation of scoring biases on aspects such as score rubrics, score IDs, and reference answer selection.},
	urldate = {2025-11-18},
	publisher = {arXiv},
	author = {Li, Qingquan and Dou, Shaoyu and Shao, Kailai and Chen, Chao and Hu, Haixiang},
	month = aug,
	year = {2025},
	note = {arXiv:2506.22316 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/JYTV7SWD/Li et al. - 2025 - Evaluating Scoring Bias in LLM-as-a-Judge.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/CPQDEM2Z/2506.html:text/html},
}

@misc{haldar_rating_2025,
	title = {Rating {Roulette}: {Self}-{Inconsistency} in {LLM}-{As}-{A}-{Judge} {Frameworks}},
	shorttitle = {Rating {Roulette}},
	url = {http://arxiv.org/abs/2510.27106},
	doi = {10.48550/arXiv.2510.27106},
	abstract = {As Natural Language Generation (NLG) continues to be widely adopted, properly assessing it has become quite difficult. Lately, using large language models (LLMs) for evaluating these generations has gained traction, as they tend to align more closely with human preferences than conventional n-gram or embedding-based metrics. In our experiments, we show that LLM judges have low intra-rater reliability in their assigned scores across different runs. This variance makes their ratings inconsistent, almost arbitrary in the worst case, making it difficult to measure how good their judgments actually are. We quantify this inconsistency across different NLG tasks and benchmarks and see if judicious use of LLM judges can still be useful following proper guidelines.},
	urldate = {2025-11-18},
	publisher = {arXiv},
	author = {Haldar, Rajarshi and Hockenmaier, Julia},
	month = oct,
	year = {2025},
	note = {arXiv:2510.27106 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at EMNLP 2025},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/H7RNMWYZ/Haldar and Hockenmaier - 2025 - Rating Roulette Self-Inconsistency in LLM-As-A-Judge Frameworks.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/YZJCXIWQ/2510.html:text/html},
}

@misc{mahdavi_refgrader_2025,
	title = {{RefGrader}: {Automated} {Grading} of {Mathematical} {Competition} {Proofs} using {Agentic} {Workflows}},
	shorttitle = {{RefGrader}},
	url = {http://arxiv.org/abs/2510.09021},
	doi = {10.48550/arXiv.2510.09021},
	abstract = {State-of-the-art (SOTA) LLMs have progressed from struggling on proof-based Olympiad problems to solving most of the IMO 2025 problems, with leading systems reportedly handling 5 of 6 problems. Given this progress, we assess how well these models can grade proofs: detecting errors, judging their severity, and assigning fair scores beyond binary correctness. We study proof-analysis capabilities using a corpus of 90 Gemini 2.5 Pro-generated solutions that we grade on a 1-4 scale with detailed error annotations, and on MathArena solution sets for IMO/USAMO 2025 scored on a 0-7 scale. Our analysis shows that models can reliably flag incorrect (including subtly incorrect) solutions but exhibit calibration gaps in how partial credit is assigned. To address this, we introduce agentic workflows that extract and analyze reference solutions and automatically derive problem-specific rubrics for a multi-step grading process. We instantiate and compare different design choices for the grading workflows, and evaluate their trade-offs. Across our annotated corpus and MathArena, our proposed workflows achieve higher agreement with human grades and more consistent handling of partial credit across metrics. We release all code, data, and prompts/logs to facilitate future research.},
	urldate = {2025-11-18},
	publisher = {arXiv},
	author = {Mahdavi, Hamed and Mahdavinia, Pouria and Malek, Samira and Mohammadipour, Pegah and Hashemi, Alireza and Daliri, Majid and Farhadi, Alireza and Khasahmadi, Amir and Mireshghallah, Niloofar and Honavar, Vasant},
	month = oct,
	year = {2025},
	note = {arXiv:2510.09021 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/YSC6X6Y5/Mahdavi et al. - 2025 - RefGrader Automated Grading of Mathematical Competition Proofs using Agentic Workflows.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/9CGSB5YK/2510.html:text/html},
}

@misc{mahdavi_scaling_2025,
	title = {Scaling {Generative} {Verifiers} {For} {Natural} {Language} {Mathematical} {Proof} {Verification} {And} {Selection}},
	url = {http://arxiv.org/abs/2511.13027},
	doi = {10.48550/arXiv.2511.13027},
	abstract = {Large language models have achieved remarkable success on final-answer mathematical problems, largely due to the ease of applying reinforcement learning with verifiable rewards. However, the reasoning underlying these solutions is often flawed. Advancing to rigorous proof-based mathematics requires reliable proof verification capabilities. We begin by analyzing multiple evaluation setups and show that focusing on a single benchmark can lead to brittle or misleading conclusions. To address this, we evaluate both proof-based and final-answer reasoning to obtain a more reliable measure of model performance. We then scale two major generative verification methods (GenSelect and LLM-as-a-Judge) to millions of tokens and identify their combination as the most effective framework for solution verification and selection. We further show that the choice of prompt for LLM-as-a-Judge significantly affects the model's performance, but reinforcement learning can reduce this sensitivity. However, despite improving proof-level metrics, reinforcement learning does not enhance final-answer precision, indicating that current models often reward stylistic or procedural correctness rather than mathematical validity. Our results establish practical guidelines for designing and evaluating scalable proof-verification and selection systems.},
	urldate = {2025-11-18},
	publisher = {arXiv},
	author = {Mahdavi, Sadegh and Kisacanin, Branislav and Toshniwal, Shubham and Du, Wei and Moshkov, Ivan and Armstrong, George and Liao, Renjie and Thrampoulidis, Christos and Gitman, Igor},
	month = nov,
	year = {2025},
	note = {arXiv:2511.13027 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/huangyanxing/Zotero/storage/TXEZPXSS/Mahdavi et al. - 2025 - Scaling Generative Verifiers For Natural Language Mathematical Proof Verification And Selection.pdf:application/pdf;Snapshot:/Users/huangyanxing/Zotero/storage/UIA7K9Q3/2511.html:text/html},
}
